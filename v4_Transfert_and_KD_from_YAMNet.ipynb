{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "v4 Transfert and KD from YAMNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kA6trXl-3zJw",
        "_UQblnrLd_ET",
        "o3whTXTH4KM1",
        "p-PuBEb6CMeo",
        "IZ7wd1263GoV",
        "Up8Xk_pMH4Rt",
        "t5McVnHmNiDw",
        "O0idLyRLQeGj",
        "g3eGPbZAyR_r",
        "3h7IcvuOOS4J",
        "sHe-Wv47rhm8",
        "HPSFmDL7pv2L",
        "yrEeXS-dUd4q"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taenor/Num_french/blob/main/v4_Transfert_and_KD_from_YAMNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA6trXl-3zJw"
      },
      "source": [
        "# I/ Preparing environment\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UQblnrLd_ET"
      },
      "source": [
        "## 1. Configure Defaults"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PYwRFppd-WB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be5641ef-319c-4fe7-cb6a-0e0d3d1f5f1e"
      },
      "source": [
        "! pip install tensorflow_io\n",
        "#! pip install tensorflow-io-nightly\n",
        "\n",
        "# Define paths to model files\n",
        "import os\n",
        "\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_io as tfio\n",
        "\n",
        "pad_z = 15600 #size of audio files used for the model \n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_io\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/b7/b76c28a422ebaf1c3d97aa6553e8620cc3b0d91976415b4ca255176c7946/tensorflow_io-0.19.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7MB)\n",
            "\u001b[K     |████████████████████████████████| 22.7MB 1.9MB/s \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem==0.19.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3a/5c1cc819ff1adfd47fa119a8b904a12207c64bdb1f61f2ef726f03a0cdc6/tensorflow_io_gcs_filesystem-0.19.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 31.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.6.0,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.7.4.3)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.15.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.12.4)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.34.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.1.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.12.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.12)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.1.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.19.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.9.2->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (57.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.31.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.8.0)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.5.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.2.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (4.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.1.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow<2.6.0,>=2.5.0->tensorflow_io) (3.4.1)\n",
            "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\n",
            "Successfully installed tensorflow-io-0.19.0 tensorflow-io-gcs-filesystem-0.19.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3whTXTH4KM1"
      },
      "source": [
        "## 2. Importing important data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKjg7QeMDsDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d655eec-e452-4278-c989-f2f04cff9764"
      },
      "source": [
        "# Import Spoken Digits Dataset\n",
        "_ = tf.keras.utils.get_file('esc-50.zip',\n",
        "                        'https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/heads/master.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='datasets',\n",
        "                        extract=True)\n",
        "\n",
        "# Import personal repo with utilities\n",
        "_ = tf.keras.utils.get_file('spoken_digits.zip',\n",
        "                        'https://github.com/Taenor/Spoken_digit_csv/archive/refs/heads/main.zip',\n",
        "                        cache_dir='./',\n",
        "                        cache_subdir='csv',\n",
        "                       extract=True)\n",
        "\n",
        "# Reorganize the files to simplifie the execution of the program\n",
        "!git clone https://github.com/tensorflow/models vggish_model \n",
        "!cp vggish_model//research/audioset/yamnet/* .\n",
        "!rm yamnet.py\n",
        "!cp csv//Spoken_digit_csv-main/yamnet.py .\n",
        "!cp csv//Spoken_digit_csv-main/yamnet_KD.py .\n",
        "!curl -O https://storage.googleapis.com/audioset/yamnet.h5"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/Jakobovski/free-spoken-digit-dataset/archive/refs/heads/master.zip\n",
            "14999552/Unknown - 1s 0us/stepDownloading data from https://github.com/Taenor/Spoken_digit_csv/archive/refs/heads/main.zip\n",
            "   8192/Unknown - 0s 0us/stepCloning into 'vggish_model'...\n",
            "remote: Enumerating objects: 58094, done.\u001b[K\n",
            "remote: Counting objects: 100% (511/511), done.\u001b[K\n",
            "remote: Compressing objects: 100% (236/236), done.\u001b[K\n",
            "remote: Total 58094 (delta 324), reused 443 (delta 275), pack-reused 57583\u001b[K\n",
            "Receiving objects: 100% (58094/58094), 573.18 MiB | 25.63 MiB/s, done.\n",
            "Resolving deltas: 100% (40272/40272), done.\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 14.5M  100 14.5M    0     0  17.5M      0 --:--:-- --:--:-- --:--:-- 17.5M\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr1VLfotanf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e05dca5-9fc0-4e4a-af8d-6cfd4dba02b4"
      },
      "source": [
        "# The Yamnet model used here is a personal Yamnet model modified from the original Github distribution\n",
        "# It doen't use the feature extractor as Input and instead directly accept batched features tensor\n",
        "# Params and features stay unchanged and will permit to manually extract features from .wav files\n",
        "\n",
        "import params\n",
        "import yamnet\n",
        "import features as features_lib\n",
        "\n",
        "yamnet_model = yamnet.yamnet_frames_model(params.Params())\n",
        "yamnet_model.load_weights('yamnet.h5')\n",
        "yamnet_classes = yamnet.class_names('yamnet_class_map.csv')\n",
        "\n",
        "# A test file from the used dataset \n",
        "testing_wav_file_name = './datasets//free-spoken-digit-dataset-master/recordings/8_yweweler_29.wav'\n",
        "\n",
        "# A test file from another dataset to test the outside compability\n",
        "testing_wav_file_name_bis = './csv//Spoken_digit_csv-main/0_Atalina_0.wav'\n",
        "\n",
        "print(testing_wav_file_name)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./datasets//free-spoken-digit-dataset-master/recordings/8_yweweler_29.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53PBJBv1jEtJ"
      },
      "source": [
        "# Util functions for loading audio files and ensure the correct sample rate and size\n",
        "\n",
        "@tf.function\n",
        "def load_wav_16k_mono(filename):\n",
        "    \"\"\" read in a waveform file and convert to 16 kHz mono \"\"\"\n",
        "    file_contents = tf.io.read_file(filename)\n",
        "    wav, sample_rate = tf.audio.decode_wav(\n",
        "          file_contents,\n",
        "          desired_channels=1)\n",
        "    wav = tf.squeeze(wav, axis=-1)\n",
        "    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n",
        "    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n",
        "    return wav\n",
        "\n",
        "def padding_vggish(audio, taille):\n",
        "  #print(testing_wav_data.numpy().size)\n",
        "\n",
        "  if (audio.numpy().size < taille):\n",
        "    padding = np.zeros(taille-audio.numpy().size)\n",
        "    audio = tf.concat([audio,padding], axis=0)\n",
        "  return audio[0:taille]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q2D0NO17swI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "fd5e74a3-ad90-4c4f-e9bd-18a70bb26a90"
      },
      "source": [
        "testing_wav_data = load_wav_16k_mono(testing_wav_file_name)\n",
        "testing_wav_data_bis = load_wav_16k_mono(testing_wav_file_name_bis)\n",
        "\n",
        "testing_wav_data = padding_vggish(testing_wav_data,pad_z)\n",
        "testing_wav_data_bis = padding_vggish(testing_wav_data_bis,pad_z)\n",
        "\n",
        "\n",
        "_ = plt.plot(testing_wav_data)\n",
        "print(tf.size(testing_wav_data))\n",
        "\n",
        "# Play the audio file.\n",
        "display.Audio(testing_wav_data,rate=16000)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/parallel_for/pfor.py:2382: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "The `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\n",
            "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n",
            "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n",
            "tf.Tensor(15600, shape=(), dtype=int32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "                <audio controls=\"controls\" >\n",
              "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
              "                    Your browser does not support the audio element.\n",
              "                </audio>\n",
              "              "
            ],
            "text/plain": [
              "<IPython.lib.display.Audio object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF6X9IP18ibA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e87e45c-654d-4a87-ea4f-a80724567ada"
      },
      "source": [
        "#Original Yamnet classes before transfert\n",
        "for name in yamnet_classes[:20]:\n",
        "  print(name)\n",
        "print('...')\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Speech\n",
            "Child speech, kid speaking\n",
            "Conversation\n",
            "Narration, monologue\n",
            "Babbling\n",
            "Speech synthesizer\n",
            "Shout\n",
            "Bellow\n",
            "Whoop\n",
            "Yell\n",
            "Children shouting\n",
            "Screaming\n",
            "Whispering\n",
            "Laughter\n",
            "Baby laughter\n",
            "Giggle\n",
            "Snicker\n",
            "Belly laugh\n",
            "Chuckle, chortle\n",
            "Crying, sobbing\n",
            "...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaaprlTM8omZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d4e03b1-1158-4f9f-80f4-3042f676504b"
      },
      "source": [
        "#Quick test of the Yamnet model\n",
        "mel,testing_wav_data = features_lib.waveform_to_log_mel_spectrogram_patches( testing_wav_data, params.Params())\n",
        "testing_wav_data =tf.reshape(testing_wav_data, (1,1,96,64))\n",
        "mel_bis,testing_wav_data_bis = features_lib.waveform_to_log_mel_spectrogram_patches( testing_wav_data_bis, params.Params())\n",
        "testing_wav_data_bis =tf.reshape(testing_wav_data_bis, (1,1,96,64))\n",
        "scores, embeddings = yamnet_model(testing_wav_data)\n",
        "class_scores = tf.reduce_mean(scores, axis=0)\n",
        "top_class = tf.argmax(class_scores)\n",
        "infered_class = yamnet_classes[top_class]\n",
        "\n",
        "print(f'The main sound is: {infered_class}')\n",
        "print(f'The embeddings shape: {embeddings.shape}')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The main sound is: Speech\n",
            "The embeddings shape: (1, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-PuBEb6CMeo"
      },
      "source": [
        "# II/Dataset\n",
        "\n",
        "I used a dataset called Spoken Diggits to try to do a transfert learning from the Yamnet model to a Diggit audio recognition model.\n",
        "\n",
        "Spoken diggits is a data set similar to Mnist but for audio. You have 3000 short audio samples of 6 different speakers, at a 8kHz sample rate (50 per speaker, per diggits).\n",
        "\n",
        "I created a .csv file with all the metadata needed and 10 equal fold categories to split the data set between train, validation and test.\n",
        "\n",
        "To realize a good Transfert learning, I normalize all audio .wav to 0.975 sec (1 Yamnet Window), I extract the right tensor from each sound file, I pass it through Yamnet and get the embedding of each sound and I split the data.\n",
        "\n",
        "A second dataset that as not been through Yamnet is also kept and split. It will be used to train the student model during KD. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ7wd1263GoV"
      },
      "source": [
        "## 1. Formating dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmvYqnikAIs5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8eff58c9-8421-43b9-93cb-0c4dd2848462"
      },
      "source": [
        "esc50_csv = './csv//Spoken_digit_csv-main/free_spoken_digits.csv'\n",
        "base_data_path = './datasets//free-spoken-digit-dataset-master/recordings/'\n",
        "\n",
        "pd_data = pd.read_csv(esc50_csv)\n",
        "pd_data.head()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>target</th>\n",
              "      <th>category</th>\n",
              "      <th>speaker</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0_george_0.wav</td>\n",
              "      <td>0.0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0_george_10.wav</td>\n",
              "      <td>0.0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0_george_11.wav</td>\n",
              "      <td>0.0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0_george_12.wav</td>\n",
              "      <td>0.0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0_george_13.wav</td>\n",
              "      <td>0.0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          Filename  target category speaker fold\n",
              "0   0_george_0.wav     0.0     zero  george    0\n",
              "1  0_george_10.wav     0.0     zero  george    0\n",
              "2  0_george_11.wav     0.0     zero  george    1\n",
              "3  0_george_12.wav     0.0     zero  george    2\n",
              "4  0_george_13.wav     0.0     zero  george    3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BABtOiMiAOxc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3f17ef4d-012a-4066-ef93-bf0787e7ee93"
      },
      "source": [
        "my_classes = ['zero', 'un', 'deux','trois','quatre','cinq','six','sept','huit','neuf']\n",
        "map_class_to_id = {'zero':0, 'un':1, 'deux':2,'trois':3,'quatre':4,'cinq':5,'six':6,'sept':7,'huit':8,'neuf':9}\n",
        "\n",
        "filtered_pd = pd_data[pd_data.category.isin(my_classes)]\n",
        "\n",
        "class_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\n",
        "filtered_pd = filtered_pd.assign(target=class_id)\n",
        "\n",
        "full_path = filtered_pd['Filename'].apply(lambda row: os.path.join(base_data_path, row))\n",
        "filtered_pd = filtered_pd.assign(Filename=full_path)\n",
        "\n",
        "filtered_pd.head(10)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Filename</th>\n",
              "      <th>target</th>\n",
              "      <th>category</th>\n",
              "      <th>speaker</th>\n",
              "      <th>fold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>./datasets//free-spoken-digit-dataset-master/r...</td>\n",
              "      <td>0</td>\n",
              "      <td>zero</td>\n",
              "      <td>george</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Filename  target  ... speaker fold\n",
              "0  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    0\n",
              "1  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    0\n",
              "2  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    1\n",
              "3  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    2\n",
              "4  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    3\n",
              "5  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    4\n",
              "6  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    5\n",
              "7  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    6\n",
              "8  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    7\n",
              "9  ./datasets//free-spoken-digit-dataset-master/r...       0  ...  george    8\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eudqonchBB-d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "389b94fe-a41f-419c-89d5-131597f2dcf6"
      },
      "source": [
        "filenames = filtered_pd['Filename']\n",
        "targets = filtered_pd['target']\n",
        "folds = filtered_pd['fold']\n",
        "\n",
        "main_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\n",
        "main_ds.element_spec\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.string, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjtQvHR7BP_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4e3e78-d6e4-4acc-d8fc-1e5c993bd2a5"
      },
      "source": [
        "def load_wav_for_map(filename, label, fold):\n",
        "  return load_wav_16k_mono(filename), label, fold\n",
        "\n",
        "main_ds = main_ds.map(load_wav_for_map)\n",
        "# for element in main_ds:\n",
        "#   _ = plt.plot(element[0])\n",
        "#   #display.Audio(element[0],rate=16000)\n",
        "\n",
        "main_ds.element_spec\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Using a while_loop for converting IO>AudioResample\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.string, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGoLQgH17h8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5464c9cf-8384-4976-f99d-758d9a9ace0b"
      },
      "source": [
        "main_ds_norm = [ ]\n",
        "for element in main_ds:\n",
        "\n",
        "  #print(\"avant : \"+str(element))\n",
        "  y = list(element)\n",
        "  y[0] = padding_vggish(element[0],pad_z)\n",
        "  element = y\n",
        "  assert element[0].numpy().size == pad_z, 'erreur de resize :'+ str(element) +str(element[0].numpy().size)\n",
        "  #print(\"apres : \"+str(element[0].numpy())+str(element[1].numpy()))\n",
        "  main_ds_norm.append((element[0].numpy(),element[1].numpy(),element[2].numpy()))\n",
        "\n",
        "print(main_ds.element_spec)\n",
        "sec_ds = tf.data.Dataset.from_tensors(main_ds_norm[0])\n",
        "for e in main_ds_norm[1:]:\n",
        "  next = tf.data.Dataset.from_tensors(e)\n",
        "  sec_ds = sec_ds.concatenate(next)\n",
        "  \n",
        "print(sec_ds.element_spec)\n",
        "main_ds = sec_ds\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))\n",
            "(TensorSpec(shape=(15600,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCtR1CpVIC-l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "882ddca6-c6b1-4f5c-e6fc-f2fb3ac15b69"
      },
      "source": [
        "def extract_features(wav_data, label, fold):\n",
        "  mel,features = features_lib.waveform_to_log_mel_spectrogram_patches( wav_data, params.Params())\n",
        "  return (features, label, fold)\n",
        "main_ds = main_ds.map(extract_features)\n",
        "print(main_ds.element_spec)\n",
        "sec_ds = main_ds #We keep a copy of the dataset before passing it through Yamnet"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(1, 96, 64), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.int64, name=None), TensorSpec(shape=(), dtype=tf.string, name=None))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFWGG9u2Bbyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2784b7fa-7491-4534-b3e6-153bbb4c66c0"
      },
      "source": [
        "# applies the embedding extraction model to a wav data\n",
        "def extract_embedding(wav_data, label, fold):\n",
        "  ''' run YAMNet to extract embedding from the wav data '''\n",
        "  num_embeddings = tf.shape(wav_data)[0]\n",
        "  wav_data = tf.reshape(wav_data, (1,num_embeddings,96,64))\n",
        "  scores, embeddings = yamnet_model(wav_data)\n",
        "  return (embeddings,\n",
        "            tf.repeat(label, num_embeddings),\n",
        "            tf.repeat(fold, num_embeddings)\n",
        "          )\n",
        "# extract embedding\n",
        "main_ds = main_ds.map(extract_embedding).unbatch()\n",
        "main_ds.element_spec\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(1024,), dtype=tf.float32, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
              " TensorSpec(shape=(), dtype=tf.string, name=None))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up8Xk_pMH4Rt"
      },
      "source": [
        "## 2. Split the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0E46bClBuNm"
      },
      "source": [
        "cached_ds = main_ds.cache()\n",
        "train_ds = cached_ds.filter(lambda embedding, label, fold: int(fold) < 8)\n",
        "val_ds = cached_ds.filter(lambda embedding, label, fold: int(fold) == 8)\n",
        "test_ds = cached_ds.filter(lambda embedding, label, fold: int(fold) == 9)\n",
        "\n",
        "#remove the folds column now that it's not needed anymore\n",
        "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
        "\n",
        "train_ds = train_ds.map(remove_fold_column)\n",
        "val_ds = val_ds.map(remove_fold_column)\n",
        "test_ds = test_ds.map(remove_fold_column)\n",
        "r_ds= cached_ds.map(remove_fold_column).batch(32)\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "siBFhLBEeiOy"
      },
      "source": [
        "cached_ds = sec_ds.cache()\n",
        "sec_train_ds = cached_ds.filter(lambda embedding, label, fold: int(fold) < 8)\n",
        "sec_val_ds = cached_ds.filter(lambda embedding, label, fold: int(fold) == 8)\n",
        "sec_test_ds = cached_ds.filter(lambda embedding, label, fold: int(fold) == 9)\n",
        "\n",
        "#remove the folds column now that it's not needed anymore\n",
        "remove_fold_column = lambda embedding, label, fold: (embedding, label)\n",
        "\n",
        "sec_train_ds = sec_train_ds.map(remove_fold_column)\n",
        "sec_val_ds = sec_val_ds.map(remove_fold_column)\n",
        "sec_test_ds = sec_test_ds.map(remove_fold_column)\n",
        "\n",
        "sec_train_ds = sec_train_ds.cache().shuffle(900).prefetch(tf.data.AUTOTUNE)\n",
        "sec_val_ds = sec_val_ds.cache().prefetch(tf.data.AUTOTUNE)\n",
        "sec_test_ds = sec_test_ds.cache().prefetch(tf.data.AUTOTUNE)\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wfdelu1TmgPk"
      },
      "source": [
        "# III/ Tranfert learning from Yamnet with a Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5McVnHmNiDw"
      },
      "source": [
        "### 1. Design the Classifier Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gD60bE8cXQId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea20274f-84cb-4609-a9ef-4307cad933e3"
      },
      "source": [
        "my_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n",
        "                          name='input_embedding'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(my_classes))\n",
        "], name='my_model')\n",
        "\n",
        "my_model.summary()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 132,490\n",
            "Trainable params: 132,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF906gj-C1sj"
      },
      "source": [
        "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                            patience=3,\n",
        "                                            restore_best_weights=True)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0idLyRLQeGj"
      },
      "source": [
        "### 2. Train the Classifier Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8hQKr4cVOdE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "387f4a99-d718-46c8-adf9-810c03c1d34f"
      },
      "source": [
        "history = my_model.fit(train_ds,\n",
        "                       epochs=50,\n",
        "                       validation_data=val_ds,\n",
        "                       callbacks=callback)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "75/75 [==============================] - 48s 392ms/step - loss: 2.1710 - accuracy: 0.2717 - val_loss: 2.0800 - val_accuracy: 0.2700\n",
            "Epoch 2/50\n",
            "75/75 [==============================] - 8s 108ms/step - loss: 1.8809 - accuracy: 0.3800 - val_loss: 1.7843 - val_accuracy: 0.3300\n",
            "Epoch 3/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 1.6281 - accuracy: 0.4550 - val_loss: 1.5385 - val_accuracy: 0.4700\n",
            "Epoch 4/50\n",
            "75/75 [==============================] - 7s 89ms/step - loss: 1.4084 - accuracy: 0.5408 - val_loss: 1.4080 - val_accuracy: 0.4900\n",
            "Epoch 5/50\n",
            "75/75 [==============================] - 7s 89ms/step - loss: 1.2565 - accuracy: 0.5833 - val_loss: 1.2620 - val_accuracy: 0.5900\n",
            "Epoch 6/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 1.1428 - accuracy: 0.6204 - val_loss: 1.1935 - val_accuracy: 0.5767\n",
            "Epoch 7/50\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 1.0334 - accuracy: 0.6633 - val_loss: 1.1509 - val_accuracy: 0.5767\n",
            "Epoch 8/50\n",
            "75/75 [==============================] - 8s 107ms/step - loss: 0.9796 - accuracy: 0.6817 - val_loss: 1.0417 - val_accuracy: 0.6633\n",
            "Epoch 9/50\n",
            "75/75 [==============================] - 8s 109ms/step - loss: 0.8823 - accuracy: 0.7146 - val_loss: 1.0203 - val_accuracy: 0.6600\n",
            "Epoch 10/50\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 0.8364 - accuracy: 0.7308 - val_loss: 0.9689 - val_accuracy: 0.6933\n",
            "Epoch 11/50\n",
            "75/75 [==============================] - 8s 107ms/step - loss: 0.7889 - accuracy: 0.7525 - val_loss: 0.8898 - val_accuracy: 0.6900\n",
            "Epoch 12/50\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.7080 - accuracy: 0.7788 - val_loss: 0.9404 - val_accuracy: 0.6667\n",
            "Epoch 13/50\n",
            "75/75 [==============================] - 8s 108ms/step - loss: 0.6997 - accuracy: 0.7850 - val_loss: 0.8480 - val_accuracy: 0.7133\n",
            "Epoch 14/50\n",
            "75/75 [==============================] - 8s 109ms/step - loss: 0.6605 - accuracy: 0.7925 - val_loss: 0.8356 - val_accuracy: 0.7333\n",
            "Epoch 15/50\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.6097 - accuracy: 0.8046 - val_loss: 0.8366 - val_accuracy: 0.7300\n",
            "Epoch 16/50\n",
            "75/75 [==============================] - 8s 109ms/step - loss: 0.5930 - accuracy: 0.8154 - val_loss: 0.7940 - val_accuracy: 0.7567\n",
            "Epoch 17/50\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.5554 - accuracy: 0.8296 - val_loss: 0.7751 - val_accuracy: 0.7533\n",
            "Epoch 18/50\n",
            "75/75 [==============================] - 8s 110ms/step - loss: 0.5256 - accuracy: 0.8458 - val_loss: 0.7468 - val_accuracy: 0.7500\n",
            "Epoch 19/50\n",
            "75/75 [==============================] - 8s 110ms/step - loss: 0.4928 - accuracy: 0.8554 - val_loss: 0.7190 - val_accuracy: 0.7633\n",
            "Epoch 20/50\n",
            "75/75 [==============================] - 8s 109ms/step - loss: 0.4732 - accuracy: 0.8417 - val_loss: 0.7185 - val_accuracy: 0.7800\n",
            "Epoch 21/50\n",
            "75/75 [==============================] - 8s 108ms/step - loss: 0.4630 - accuracy: 0.8558 - val_loss: 0.6952 - val_accuracy: 0.7633\n",
            "Epoch 22/50\n",
            "75/75 [==============================] - 8s 107ms/step - loss: 0.4254 - accuracy: 0.8712 - val_loss: 0.6746 - val_accuracy: 0.7767\n",
            "Epoch 23/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.4015 - accuracy: 0.8821 - val_loss: 0.6598 - val_accuracy: 0.7867\n",
            "Epoch 24/50\n",
            "75/75 [==============================] - 8s 105ms/step - loss: 0.3768 - accuracy: 0.8875 - val_loss: 0.6709 - val_accuracy: 0.7767\n",
            "Epoch 25/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.3562 - accuracy: 0.8996 - val_loss: 0.6528 - val_accuracy: 0.7833\n",
            "Epoch 26/50\n",
            "75/75 [==============================] - 6s 87ms/step - loss: 0.3381 - accuracy: 0.8992 - val_loss: 0.6491 - val_accuracy: 0.7700\n",
            "Epoch 27/50\n",
            "75/75 [==============================] - 8s 103ms/step - loss: 0.3222 - accuracy: 0.9112 - val_loss: 0.6453 - val_accuracy: 0.8000\n",
            "Epoch 28/50\n",
            "75/75 [==============================] - 8s 105ms/step - loss: 0.3104 - accuracy: 0.9100 - val_loss: 0.6334 - val_accuracy: 0.7833\n",
            "Epoch 29/50\n",
            "75/75 [==============================] - 8s 103ms/step - loss: 0.2997 - accuracy: 0.9167 - val_loss: 0.6347 - val_accuracy: 0.7933\n",
            "Epoch 30/50\n",
            "75/75 [==============================] - 8s 108ms/step - loss: 0.2824 - accuracy: 0.9262 - val_loss: 0.6154 - val_accuracy: 0.8033\n",
            "Epoch 31/50\n",
            "75/75 [==============================] - 7s 89ms/step - loss: 0.2668 - accuracy: 0.9292 - val_loss: 0.6104 - val_accuracy: 0.8033\n",
            "Epoch 32/50\n",
            "75/75 [==============================] - 8s 110ms/step - loss: 0.2545 - accuracy: 0.9337 - val_loss: 0.5988 - val_accuracy: 0.8100\n",
            "Epoch 33/50\n",
            "75/75 [==============================] - 7s 91ms/step - loss: 0.2372 - accuracy: 0.9383 - val_loss: 0.6058 - val_accuracy: 0.8000\n",
            "Epoch 34/50\n",
            "75/75 [==============================] - 8s 107ms/step - loss: 0.2300 - accuracy: 0.9400 - val_loss: 0.6390 - val_accuracy: 0.8033\n",
            "Epoch 35/50\n",
            "75/75 [==============================] - 7s 88ms/step - loss: 0.2188 - accuracy: 0.9433 - val_loss: 0.6221 - val_accuracy: 0.8000\n",
            "Epoch 36/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.2143 - accuracy: 0.9450 - val_loss: 0.6093 - val_accuracy: 0.8267\n",
            "Epoch 37/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.2089 - accuracy: 0.9417 - val_loss: 0.5989 - val_accuracy: 0.8067\n",
            "Epoch 38/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.1968 - accuracy: 0.9550 - val_loss: 0.5752 - val_accuracy: 0.8067\n",
            "Epoch 39/50\n",
            "75/75 [==============================] - 8s 105ms/step - loss: 0.1791 - accuracy: 0.9596 - val_loss: 0.5959 - val_accuracy: 0.8167\n",
            "Epoch 40/50\n",
            "75/75 [==============================] - 8s 107ms/step - loss: 0.1683 - accuracy: 0.9625 - val_loss: 0.5862 - val_accuracy: 0.8067\n",
            "Epoch 41/50\n",
            "75/75 [==============================] - 8s 105ms/step - loss: 0.1727 - accuracy: 0.9608 - val_loss: 0.6005 - val_accuracy: 0.8100\n",
            "Epoch 42/50\n",
            "75/75 [==============================] - 7s 90ms/step - loss: 0.1563 - accuracy: 0.9658 - val_loss: 0.5909 - val_accuracy: 0.8167\n",
            "Epoch 43/50\n",
            "75/75 [==============================] - 6s 88ms/step - loss: 0.1512 - accuracy: 0.9671 - val_loss: 0.5834 - val_accuracy: 0.8100\n",
            "Epoch 44/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.1382 - accuracy: 0.9721 - val_loss: 0.6033 - val_accuracy: 0.8200\n",
            "Epoch 45/50\n",
            "75/75 [==============================] - 7s 88ms/step - loss: 0.1404 - accuracy: 0.9712 - val_loss: 0.5704 - val_accuracy: 0.8167\n",
            "Epoch 46/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.1271 - accuracy: 0.9725 - val_loss: 0.5749 - val_accuracy: 0.8133\n",
            "Epoch 47/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.1281 - accuracy: 0.9737 - val_loss: 0.5814 - val_accuracy: 0.8133\n",
            "Epoch 48/50\n",
            "75/75 [==============================] - 7s 89ms/step - loss: 0.1149 - accuracy: 0.9783 - val_loss: 0.5771 - val_accuracy: 0.8167\n",
            "Epoch 49/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.1110 - accuracy: 0.9767 - val_loss: 0.5725 - val_accuracy: 0.8067\n",
            "Epoch 50/50\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 0.1051 - accuracy: 0.9767 - val_loss: 0.5847 - val_accuracy: 0.8300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4sCiqdJDUnA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc030e9e-8d0f-40ba-cab0-fa23adab790c"
      },
      "source": [
        "loss, accuracy = my_model.evaluate(test_ds)\n",
        "\n",
        "print(\"Loss: \", loss)\n",
        "print(\"Accuracy: \", accuracy)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 7s 18ms/step - loss: 0.4815 - accuracy: 0.8400\n",
            "Loss:  0.4815121591091156\n",
            "Accuracy:  0.8399999737739563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3eGPbZAyR_r"
      },
      "source": [
        "### 3. Test the Classifier model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHyNO_6ADvSO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7748a402-7c55-41f4-b28d-096302a59e46"
      },
      "source": [
        "scores, embeddings = yamnet_model(testing_wav_data)\n",
        "result = my_model(embeddings).numpy()\n",
        "\n",
        "class_scores = tf.reduce_mean(result, axis=0)\n",
        "top_class = tf.argmax(class_scores)\n",
        "infered_class = my_classes[top_class]\n",
        "#infered_class = my_classes[result.mean(axis=0).argmax()]\n",
        "#print(result)\n",
        "print(f'The main sound is: {infered_class}')\n",
        "\n",
        "scores, embeddings = yamnet_model(testing_wav_data_bis)\n",
        "result = my_model(embeddings).numpy()\n",
        "\n",
        "class_scores = tf.reduce_mean(result, axis=0)\n",
        "top_class = tf.argmax(class_scores)\n",
        "infered_class = my_classes[top_class]\n",
        "#infered_class = my_classes[result.mean(axis=0).argmax()]\n",
        "#print(result)\n",
        "print(f'The main sound is: {infered_class}')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The main sound is: huit\n",
            "The main sound is: zero\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYHGswAJJgrC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "30ba3d39-0c7c-42e8-e9ac-3140a7c5e67e"
      },
      "source": [
        "# Draw a graph of the loss, which is the distance between\n",
        "# the predicted and actual values during training and validation.\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "# Exclude the first few epochs so the graph is easier to read\n",
        "SKIP = 5\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "\n",
        "plt.plot(epochs[SKIP:], train_loss[SKIP:], 'g.', label='Training loss')\n",
        "plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "\n",
        "# Draw a graph of mean absolute error, which is another way of\n",
        "# measuring the amount of error in the prediction.\n",
        "train_mae = history.history['accuracy']\n",
        "val_mae = history.history['val_accuracy']\n",
        "\n",
        "plt.plot(epochs[SKIP:], train_mae[SKIP:], 'g.', label='Training MAE')\n",
        "plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')\n",
        "plt.title('Training and accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3h7IcvuOOS4J"
      },
      "source": [
        "## 4. Generate a TensorFlow Lite Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHe-Wv47rhm8"
      },
      "source": [
        "### a) Generate Models with or without Quantization\n",
        "We now have an acceptably accurate model. We'll use the [TensorFlow Lite Converter](https://www.tensorflow.org/lite/convert) to convert the model into a special, space-efficient format for use on memory-constrained devices.\n",
        "\n",
        "Since this model is going to be deployed on a microcontroller, we want it to be as tiny as possible! One technique for reducing the size of a model is called [quantization](https://www.tensorflow.org/lite/performance/post_training_quantization). It reduces the precision of the model's weights, and possibly the activations (output of each layer) as well, which saves memory, often without much impact on accuracy. Quantized models also run faster, since the calculations required are simpler.\n",
        "\n",
        "In the following cell, we'll convert the model twice: once with quantization, once without."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUVCI2Suunpw"
      },
      "source": [
        "class ReduceMeanLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, axis=0, **kwargs):\n",
        "    super(ReduceMeanLayer, self).__init__(**kwargs)\n",
        "    self.axis = axis\n",
        "\n",
        "  def call(self, input):\n",
        "    return tf.math.reduce_mean(input, axis=self.axis)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zE_Npm0nzlwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e4a4f18-aac8-4a66-bba9-804e326247c3"
      },
      "source": [
        "saved_model_path = './digits_yamnet'\n",
        "\n",
        "input_segment = tf.keras.layers.Input(shape=(None,96,64), dtype=tf.float32, name='audio')\n",
        "embeddings_output = yamnet_model(input_segment)\n",
        "serving_outputs = my_model(embeddings_output[1])\n",
        "#serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
        "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
        "serving_model.save(saved_model_path, include_optimizer=False)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "INFO:tensorflow:Assets written to: ./digits_yamnet/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDwppVoXF_bM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "97f3a3a9-ce59-4ab3-c399-c44d2b00a9c2"
      },
      "source": [
        "tf.keras.utils.plot_model(serving_model)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkYVpJS72WWB"
      },
      "source": [
        "#reloaded_model = tf.saved_model.load(saved_model_path)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT3VyCqeHGpv"
      },
      "source": [
        "MODELS_DIR = 'models/'\n",
        "if not os.path.exists(MODELS_DIR):\n",
        "    os.mkdir(MODELS_DIR)\n",
        "MODEL_TF = MODELS_DIR + 'model'\n",
        "MODEL_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'\n",
        "MODEL_TFLITE = MODELS_DIR + 'model.tflite'\n",
        "MODEL_TFLITE_MICRO = MODELS_DIR + 'model.cc'\n",
        "KD_MODEL_TF = MODELS_DIR + 'KD_model'\n",
        "KD_MODEL_NO_QUANT_TFLITE =  MODELS_DIR + 'KD_model_no_quant.tflite'\n",
        "KD_MODEL_TFLITE = MODELS_DIR + 'KD_model.tflite'\n",
        "KD_MODEL_TFLITE_MICRO = MODELS_DIR + 'KD_model.cc'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1muAoUm8lSXL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ccfea2-7a2b-4cc6-c773-617910216290"
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "model_no_quant_tflite = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(MODEL_NO_QUANT_TFLITE, \"wb\").write(model_no_quant_tflite)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13334616"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQF0PLcCXPAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a229144-5e08-4fd6-c1ff-1793b3d2934a"
      },
      "source": [
        "\n",
        "# Convert the model to the TensorFlow Lite format with quantization\n",
        "print(sec_ds.batch(32).element_spec)\n",
        "def representative_dataset():\n",
        "  for data in sec_ds.batch(32):\n",
        "    yield [tf.dtypes.cast(data[0], tf.float32)]\n",
        "# Set the optimization flag.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Enforce integer only quantization\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "# Provide a representative dataset to ensure we quantize correctly.\n",
        "converter.representative_dataset = representative_dataset#representative_dataset\n",
        "model_tflite = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(MODEL_TFLITE, \"wb\").write(model_tflite)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(None, 1, 96, 64), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3649104"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPSFmDL7pv2L"
      },
      "source": [
        "### b) Generate a TensorFlow Lite for Microcontrollers Model\n",
        "Convert the TensorFlow Lite quantized model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1FB4ieeg0lw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd4da2ff-0d00-42ad-bfac-230f0990d578"
      },
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {MODEL_TFLITE_MICRO}"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (91.189.91.39)] [Connected to cloud.r-pro\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [2 InRelease 15.6 kB/88.7 kB 18%] [Connecting to security.ubuntu.com (91.189\r0% [1 InRelease gpgv 242 kB] [2 InRelease 15.6 kB/88.7 kB 18%] [Connecting to s\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Waiti\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,188 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,656 kB]\n",
            "Ign:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [506 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,777 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [909 kB]\n",
            "Get:20 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [41.5 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,221 kB]\n",
            "Ign:24 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:24 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [634 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,418 kB]\n",
            "Get:26 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [473 kB]\n",
            "Fetched 13.2 MB in 3s (4,096 kB/s)\n",
            "Reading package lists... Done\n",
            "Selecting previously unselected package xxd.\n",
            "(Reading database ... 160772 files and directories currently installed.)\n",
            "Preparing to unpack .../xxd_2%3a8.0.1453-1ubuntu1.4_amd64.deb ...\n",
            "Unpacking xxd (2:8.0.1453-1ubuntu1.4) ...\n",
            "Setting up xxd (2:8.0.1453-1ubuntu1.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WCmWgEB4mAf"
      },
      "source": [
        "# IV/ Knowledge Distillation and Quantization of Complete Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aohwghOt4WH"
      },
      "source": [
        "## 1. Distillation\n",
        "\n",
        "We create a new model inspired by the original **Yamnet** model and realize a cross training of this small model to teach him to imitate the bigger model trained above.\n",
        "\n",
        "I followed a method of training used for embedding **VGGish** (another audio analysis model, develloped by the same team than Yamnet). You can find this method in this article : \n",
        "\n",
        "https://arxiv.org/pdf/2001.10876.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrEeXS-dUd4q"
      },
      "source": [
        "### Distiller Class and Student model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS2Sr4rZtJ2k"
      },
      "source": [
        "class Distiller(tf.keras.Model):\n",
        "    def __init__(self, student, teacher, teacher_embed):\n",
        "        super(Distiller, self).__init__()\n",
        "        self.teacher = teacher\n",
        "        self.student = student\n",
        "        self.teacher_embed = teacher_embed\n",
        "\n",
        "    def compile(\n",
        "        self,\n",
        "        optimizer,\n",
        "        metrics,\n",
        "        student_loss_fn,\n",
        "        distillation_loss_fn,\n",
        "        alpha=0.1,\n",
        "        temperature=3,\n",
        "    ):\n",
        "        \"\"\" Configure the distiller.\n",
        "\n",
        "        Args:\n",
        "            optimizer: Keras optimizer for the student weights\n",
        "            metrics: Keras metrics for evaluation\n",
        "            student_loss_fn: Loss function of difference between student\n",
        "                predictions and ground-truth\n",
        "            distillation_loss_fn: Loss function of difference between soft\n",
        "                student predictions and soft teacher predictions\n",
        "            alpha: weight to student_loss_fn and 1-alpha to distillation_loss_fn\n",
        "            temperature: Temperature for softening probability distributions.\n",
        "                Larger temperature gives softer distributions.\n",
        "        \"\"\"\n",
        "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics)\n",
        "        self.student_loss_fn = student_loss_fn\n",
        "        self.distillation_loss_fn = distillation_loss_fn\n",
        "        self.alpha = alpha\n",
        "        self.temperature = temperature\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Unpack data\n",
        "        x, y = data\n",
        "\n",
        "        # Forward pass of teacher\n",
        "        teacher_predictions = self.teacher(x, training=False)\n",
        "        teacher_embed_predictions = self.teacher_embed(x, training=False)[1]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Forward pass of student\n",
        "            student_predictions,student_embed_predictions = self.student(x, training=True)\n",
        "\n",
        "            # Compute losses\n",
        "            student_loss = self.student_loss_fn(y, student_predictions)\n",
        "            distillation_loss = self.distillation_loss_fn(\n",
        "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
        "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
        "            )\n",
        "            embed_loss =  tf.keras.backend.sum(tf.keras.backend.square(teacher_embed_predictions - student_embed_predictions), axis=-1)\n",
        "            loss = self.alpha * student_loss + (1 - self.alpha)/2 * distillation_loss + (1 - self.alpha)/2 * embed_loss\n",
        "\n",
        "        # Compute gradients\n",
        "        trainable_vars = self.student.trainable_variables\n",
        "        gradients = tape.gradient(loss, trainable_vars)\n",
        "\n",
        "        # Update weights\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
        "\n",
        "        # Update the metrics configured in `compile()`.\n",
        "        self.compiled_metrics.update_state(y, student_predictions)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update(\n",
        "            {\"sd_loss\": student_loss, \"dst_loss\": distillation_loss, \"embed_loss\": embed_loss}\n",
        "        )\n",
        "        return results\n",
        "\n",
        "    def test_step(self, data):\n",
        "        # Unpack the data\n",
        "        x, y = data\n",
        "\n",
        "        # Compute predictions\n",
        "        y_prediction = self.student(x, training=False)[0]\n",
        "\n",
        "        # Calculate the loss\n",
        "        student_loss = self.student_loss_fn(y, y_prediction)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(y, y_prediction)\n",
        "\n",
        "        # Return a dict of performance\n",
        "        results = {m.name: m.result() for m in self.metrics}\n",
        "        results.update({\"student_loss\": student_loss})\n",
        "        return results\n",
        "\n",
        "    def call(self, inputs):\n",
        "      \n",
        "        student_predictions,student_embed_predictions = self.student(inputs)\n",
        "        return student_predictions,student_embed_predictions\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_CeT-hN41LP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7155fe01-c2be-42d5-cd81-9be34a03699b"
      },
      "source": [
        "# Create the teacher\n",
        "teacher = serving_model\n",
        "\n",
        "# Create the student\n",
        "import yamnet_KD as stud\n",
        "pad = params.Params().conv_padding\n",
        "student_embed = tf.keras.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Input(shape=(None,1,96,64), dtype=tf.float32, name='audio'),\n",
        "        tf.keras.layers.Reshape((96,64,1)),\n",
        "        tf.keras.layers.Conv2D(2, (3,3), strides=2, padding=pad,use_bias=False,activation=None,name='Conv1'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm0',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ0'),\n",
        "     \n",
        "        tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3), strides=1, depth_multiplier=1,padding=pad,use_bias=False,activation=None,data_format='channels_last',name='depthConv1'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm1',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ1'),\n",
        "        tf.keras.layers.Conv2D(4, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv2'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm2',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ2'),\n",
        "\n",
        "        tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=2, depth_multiplier=1, padding=pad, use_bias=False,activation=None,data_format='channels_last',name='depthConv2'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm3',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ3'),\n",
        "        tf.keras.layers.Conv2D(8, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv3'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm4',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ4'),\n",
        "     \n",
        "        # tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=1, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv3'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm5',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ5'),\n",
        "        # tf.keras.layers.Conv2D(32, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv4'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm6',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ6'),\n",
        "     \n",
        "        tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=2, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv4'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm7',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ7'),\n",
        "        tf.keras.layers.Conv2D(16, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv5'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm8',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ8'),\n",
        "     \n",
        "        # tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=1, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv5'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm9',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ9'),\n",
        "        # tf.keras.layers.Conv2D(32, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv6'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm10',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ10'),\n",
        "     \n",
        "        # tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=2, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv6'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm11',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ11'),\n",
        "        # tf.keras.layers.Conv2D(32, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv7'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm12',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ12'),\n",
        "     \n",
        "        tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=1, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv7'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm13',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ13'),\n",
        "        tf.keras.layers.Conv2D(32, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv8'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm14'),\n",
        "        tf.keras.layers.ReLU(name='Activ14'),\n",
        "     \n",
        "        # tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=1, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv8'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm15',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ15'),\n",
        "        # tf.keras.layers.Conv2D(64, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv9'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm16',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ16'),\n",
        "     \n",
        "        # tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=1, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv9'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm17',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ17'),\n",
        "        # tf.keras.layers.Conv2D(64, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv10'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm18',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ18'),\n",
        "     \n",
        "        # tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=1, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv10'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm19',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ19'),\n",
        "        # tf.keras.layers.Conv2D(64, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv11'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm20',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ20'),\n",
        "     \n",
        "        # tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=1, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv11'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm21',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ21'),\n",
        "        # tf.keras.layers.Conv2D(64, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv12'),\n",
        "        # tf.keras.layers.BatchNormalization(name='Norm22',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        # tf.keras.layers.ReLU(name='Activ22'),\n",
        "     \n",
        "        tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=2, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv12'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm23',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ23'),\n",
        "        tf.keras.layers.Conv2D(64, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv13'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm24',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ24'),\n",
        "     \n",
        "        tf.keras.layers.DepthwiseConv2D(kernel_size=(3,3),strides=1, depth_multiplier=1, padding=pad, use_bias=False,data_format='channels_last',activation=None,name='depthConv13'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm25',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ25'),\n",
        "        tf.keras.layers.Conv2D(1024, kernel_size=(1,1), strides=1, padding=pad,use_bias=False,activation=None,name='Conv14'),\n",
        "        tf.keras.layers.BatchNormalization(name='Norm26',center=params.Params().batchnorm_center, scale=params.Params().batchnorm_scale, epsilon=params.Params().batchnorm_epsilon),\n",
        "        tf.keras.layers.ReLU(name='Activ26'),\n",
        "     \n",
        "        tf.keras.layers.GlobalAveragePooling2D(name='AveragePool'),\n",
        "\n",
        "        # tf.keras.layers.Dense(128),\n",
        "        # tf.keras.layers.Dense(10),\n",
        "    ],\n",
        "    name=\"student_embed\",\n",
        ")\n",
        "\n",
        "#student_embed.summary()\n",
        "\n",
        "def student(features):\n",
        "  embeddings = student_embed(features)\n",
        "  classifier = tf.keras.layers.Dense(64)(embeddings)\n",
        "  logits = tf.keras.layers.Dense(10)(embeddings)\n",
        "  return logits, embeddings\n",
        "\n",
        "\n",
        "def student_model():\n",
        "  features = tf.keras.layers.Input(batch_shape=(None,1,96,64), dtype=tf.float32)\n",
        "  logits, embeddings = student(features)\n",
        "  student_model = tf.keras.Model(name='student', inputs=features, outputs=[logits,embeddings])\n",
        "  return student_model\n",
        "\n",
        "\n",
        "KD_student = student_model()\n",
        "#student =  stud.yamnet_frames_model(params.Params())       #hub.load(yamnet_model_handle)\n",
        "# Clone student for later comparison\n",
        "student_scratch = tf.keras.models.clone_model(KD_student)\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_dX2iHtV_Mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972b2a65-6534-4c82-ef63-cdb0d51c4b79"
      },
      "source": [
        "teacher.summary()\n",
        "KD_student.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "audio (InputLayer)           [(None, None, 96, 64)]    0         \n",
            "_________________________________________________________________\n",
            "yamnet_frames (Functional)   [(None, 521), (None, 1024 3751369   \n",
            "_________________________________________________________________\n",
            "my_model (Sequential)        (None, 10)                132490    \n",
            "=================================================================\n",
            "Total params: 3,883,859\n",
            "Trainable params: 3,861,971\n",
            "Non-trainable params: 21,888\n",
            "_________________________________________________________________\n",
            "Model: \"student\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 1, 96, 64)]       0         \n",
            "_________________________________________________________________\n",
            "student_embed (Sequential)   (None, 1024)              73276     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                10250     \n",
            "=================================================================\n",
            "Total params: 83,526\n",
            "Trainable params: 80,974\n",
            "Non-trainable params: 2,552\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrGZ5eELUp8X"
      },
      "source": [
        "###KD training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV4U7uWTvVnZ"
      },
      "source": [
        "Firslty, we train a copy of the student model from scratch using the same data to compare the result of a normal training to the result of a KD training.\n",
        "\n",
        "The second step is to use the Distiller class to train our student model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Csis8X5Qy6U3",
        "outputId": "8fe85db1-7040-40ab-db5d-3c0ef99de7dc"
      },
      "source": [
        "student_scratch.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                            patience=3,\n",
        "                                            restore_best_weights=True)\n",
        "\n",
        "history = student_scratch.fit(sec_train_ds.batch(32),\n",
        "                       epochs=20,\n",
        "                       validation_data=sec_val_ds.batch(32),\n",
        "                       callbacks=callback)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "     74/Unknown - 19s 116ms/step - loss: 9.3649 - dense_4_loss: 2.4932 - student_embed_loss: 6.8717 - dense_4_accuracy: 0.2149 - student_embed_accuracy: 0.0110WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "75/75 [==============================] - 28s 235ms/step - loss: 9.3582 - dense_4_loss: 2.4877 - student_embed_loss: 6.8705 - dense_4_accuracy: 0.2150 - student_embed_accuracy: 0.0113 - val_loss: 9.2275 - val_dense_4_loss: 2.3102 - val_student_embed_loss: 6.9172 - val_dense_4_accuracy: 0.1000 - val_student_embed_accuracy: 0.0000e+00\n",
            "Epoch 2/20\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 9.0114 - dense_4_loss: 2.2702 - student_embed_loss: 6.7412 - dense_4_accuracy: 0.2554 - student_embed_accuracy: 0.0696 - val_loss: 9.2522 - val_dense_4_loss: 2.3275 - val_student_embed_loss: 6.9247 - val_dense_4_accuracy: 0.1000 - val_student_embed_accuracy: 0.0000e+00\n",
            "Epoch 3/20\n",
            "75/75 [==============================] - 9s 122ms/step - loss: 8.6592 - dense_4_loss: 2.0410 - student_embed_loss: 6.6182 - dense_4_accuracy: 0.3129 - student_embed_accuracy: 0.1521 - val_loss: 9.2795 - val_dense_4_loss: 2.3358 - val_student_embed_loss: 6.9437 - val_dense_4_accuracy: 0.1000 - val_student_embed_accuracy: 0.0000e+00\n",
            "Epoch 4/20\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 8.2268 - dense_4_loss: 1.7410 - student_embed_loss: 6.4858 - dense_4_accuracy: 0.4013 - student_embed_accuracy: 0.2329 - val_loss: 9.2309 - val_dense_4_loss: 2.3177 - val_student_embed_loss: 6.9132 - val_dense_4_accuracy: 0.1267 - val_student_embed_accuracy: 0.0000e+00\n",
            "Epoch 5/20\n",
            "75/75 [==============================] - 9s 121ms/step - loss: 7.9183 - dense_4_loss: 1.5724 - student_embed_loss: 6.3459 - dense_4_accuracy: 0.4588 - student_embed_accuracy: 0.3317 - val_loss: 8.9282 - val_dense_4_loss: 2.1238 - val_student_embed_loss: 6.8044 - val_dense_4_accuracy: 0.2000 - val_student_embed_accuracy: 0.0033\n",
            "Epoch 6/20\n",
            "75/75 [==============================] - 8s 105ms/step - loss: 7.5442 - dense_4_loss: 1.3308 - student_embed_loss: 6.2134 - dense_4_accuracy: 0.5479 - student_embed_accuracy: 0.4175 - val_loss: 8.2701 - val_dense_4_loss: 1.7010 - val_student_embed_loss: 6.5691 - val_dense_4_accuracy: 0.4200 - val_student_embed_accuracy: 0.2000\n",
            "Epoch 7/20\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 7.2341 - dense_4_loss: 1.1518 - student_embed_loss: 6.0823 - dense_4_accuracy: 0.6021 - student_embed_accuracy: 0.4871 - val_loss: 7.6709 - val_dense_4_loss: 1.4084 - val_student_embed_loss: 6.2625 - val_dense_4_accuracy: 0.4600 - val_student_embed_accuracy: 0.3400\n",
            "Epoch 8/20\n",
            "75/75 [==============================] - 8s 106ms/step - loss: 6.9582 - dense_4_loss: 0.9931 - student_embed_loss: 5.9652 - dense_4_accuracy: 0.6587 - student_embed_accuracy: 0.5392 - val_loss: 7.1138 - val_dense_4_loss: 1.1599 - val_student_embed_loss: 5.9539 - val_dense_4_accuracy: 0.5767 - val_student_embed_accuracy: 0.5100\n",
            "Epoch 9/20\n",
            "75/75 [==============================] - 8s 108ms/step - loss: 6.7345 - dense_4_loss: 0.8777 - student_embed_loss: 5.8568 - dense_4_accuracy: 0.6946 - student_embed_accuracy: 0.5850 - val_loss: 7.2189 - val_dense_4_loss: 1.3129 - val_student_embed_loss: 5.9060 - val_dense_4_accuracy: 0.5567 - val_student_embed_accuracy: 0.3833\n",
            "Epoch 10/20\n",
            "75/75 [==============================] - 9s 123ms/step - loss: 6.4894 - dense_4_loss: 0.7542 - student_embed_loss: 5.7351 - dense_4_accuracy: 0.7408 - student_embed_accuracy: 0.6300 - val_loss: 6.6421 - val_dense_4_loss: 0.9684 - val_student_embed_loss: 5.6737 - val_dense_4_accuracy: 0.6533 - val_student_embed_accuracy: 0.5333\n",
            "Epoch 11/20\n",
            "75/75 [==============================] - 8s 105ms/step - loss: 6.2776 - dense_4_loss: 0.6592 - student_embed_loss: 5.6184 - dense_4_accuracy: 0.7717 - student_embed_accuracy: 0.6471 - val_loss: 6.5148 - val_dense_4_loss: 1.0330 - val_student_embed_loss: 5.4818 - val_dense_4_accuracy: 0.6300 - val_student_embed_accuracy: 0.6000\n",
            "Epoch 12/20\n",
            "75/75 [==============================] - 9s 123ms/step - loss: 6.0542 - dense_4_loss: 0.5529 - student_embed_loss: 5.5012 - dense_4_accuracy: 0.8054 - student_embed_accuracy: 0.6767 - val_loss: 6.0637 - val_dense_4_loss: 0.7365 - val_student_embed_loss: 5.3272 - val_dense_4_accuracy: 0.7433 - val_student_embed_accuracy: 0.6633\n",
            "Epoch 13/20\n",
            "75/75 [==============================] - 8s 108ms/step - loss: 5.9402 - dense_4_loss: 0.5211 - student_embed_loss: 5.4191 - dense_4_accuracy: 0.8192 - student_embed_accuracy: 0.6825 - val_loss: 5.8778 - val_dense_4_loss: 0.6554 - val_student_embed_loss: 5.2224 - val_dense_4_accuracy: 0.7933 - val_student_embed_accuracy: 0.7067\n",
            "Epoch 14/20\n",
            "75/75 [==============================] - 9s 124ms/step - loss: 5.7682 - dense_4_loss: 0.4471 - student_embed_loss: 5.3211 - dense_4_accuracy: 0.8462 - student_embed_accuracy: 0.7067 - val_loss: 5.7959 - val_dense_4_loss: 0.6277 - val_student_embed_loss: 5.1682 - val_dense_4_accuracy: 0.8100 - val_student_embed_accuracy: 0.6667\n",
            "Epoch 15/20\n",
            "75/75 [==============================] - 10s 128ms/step - loss: 5.6128 - dense_4_loss: 0.3871 - student_embed_loss: 5.2257 - dense_4_accuracy: 0.8650 - student_embed_accuracy: 0.7375 - val_loss: 5.5583 - val_dense_4_loss: 0.5171 - val_student_embed_loss: 5.0411 - val_dense_4_accuracy: 0.8367 - val_student_embed_accuracy: 0.7233\n",
            "Epoch 16/20\n",
            "75/75 [==============================] - 8s 108ms/step - loss: 5.5369 - dense_4_loss: 0.3781 - student_embed_loss: 5.1587 - dense_4_accuracy: 0.8725 - student_embed_accuracy: 0.7304 - val_loss: 5.3477 - val_dense_4_loss: 0.4657 - val_student_embed_loss: 4.8821 - val_dense_4_accuracy: 0.8267 - val_student_embed_accuracy: 0.7800\n",
            "Epoch 17/20\n",
            "75/75 [==============================] - 9s 121ms/step - loss: 5.4046 - dense_4_loss: 0.3323 - student_embed_loss: 5.0723 - dense_4_accuracy: 0.8946 - student_embed_accuracy: 0.7504 - val_loss: 5.3155 - val_dense_4_loss: 0.4682 - val_student_embed_loss: 4.8473 - val_dense_4_accuracy: 0.8367 - val_student_embed_accuracy: 0.8100\n",
            "Epoch 18/20\n",
            "75/75 [==============================] - 9s 122ms/step - loss: 5.2880 - dense_4_loss: 0.2932 - student_embed_loss: 4.9949 - dense_4_accuracy: 0.9042 - student_embed_accuracy: 0.7558 - val_loss: 5.2994 - val_dense_4_loss: 0.4791 - val_student_embed_loss: 4.8203 - val_dense_4_accuracy: 0.8467 - val_student_embed_accuracy: 0.7400\n",
            "Epoch 19/20\n",
            "75/75 [==============================] - 9s 125ms/step - loss: 5.1712 - dense_4_loss: 0.2619 - student_embed_loss: 4.9093 - dense_4_accuracy: 0.9187 - student_embed_accuracy: 0.7608 - val_loss: 5.1402 - val_dense_4_loss: 0.4501 - val_student_embed_loss: 4.6901 - val_dense_4_accuracy: 0.8567 - val_student_embed_accuracy: 0.8267\n",
            "Epoch 20/20\n",
            "75/75 [==============================] - 9s 122ms/step - loss: 5.1146 - dense_4_loss: 0.2760 - student_embed_loss: 4.8386 - dense_4_accuracy: 0.9100 - student_embed_accuracy: 0.7663 - val_loss: 4.9229 - val_dense_4_loss: 0.3335 - val_student_embed_loss: 4.5894 - val_dense_4_accuracy: 0.9100 - val_student_embed_accuracy: 0.8500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pag24eavs8u7",
        "outputId": "7d3ee20f-4b94-4a39-fc49-359f4e397819"
      },
      "source": [
        "student_scratch.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                 optimizer=\"adam\",\n",
        "                 metrics=['accuracy'])\n",
        "\n",
        "student_scratch.evaluate(sec_test_ds.batch(32))\n",
        "\n",
        "# for e in sec_test_ds.batch(1):\n",
        "#   label = e[1].numpy()\n",
        "#   scores = student_scratch(e[0])[0]\n",
        "#   class_scores = tf.reduce_mean(scores, axis=0)\n",
        "#   #print(class_scores)\n",
        "#   top_class = tf.argmax(class_scores)\n",
        "#   print(f'Je trouve {top_class} reponse etait {label}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "10/10 [==============================] - 9s 25ms/step - loss: 4.9538 - dense_4_loss: 0.3351 - student_embed_loss: 4.6187 - dense_4_accuracy: 0.8667 - student_embed_accuracy: 0.8133\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.953803539276123,\n",
              " 0.3351117968559265,\n",
              " 4.618691921234131,\n",
              " 0.8666666746139526,\n",
              " 0.8133333325386047]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkuLi_Kf-9fK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38954356-f971-490d-d90f-86f10a37e894"
      },
      "source": [
        "# Initialize and compile distiller\n",
        "distiller = Distiller(student=KD_student, teacher=teacher, teacher_embed=yamnet_model)\n",
        "distiller.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
        "    student_loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    distillation_loss_fn=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "    alpha=0.1,\n",
        "    temperature=5,\n",
        ")\n",
        "\n",
        "# Distill teacher to student\n",
        "distiller.fit(sec_train_ds.batch(32), epochs=150,validation_data=sec_val_ds.batch(32))\n",
        "\n",
        "# Evaluate student on test dataset\n",
        "distiller.evaluate(sec_test_ds.batch(32))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "     75/Unknown - 7s 31ms/step - sparse_categorical_accuracy: 0.1587 - sd_loss: 2.4365 - dst_loss: 2.3004 - embed_loss: 63.8797WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "75/75 [==============================] - 14s 129ms/step - sparse_categorical_accuracy: 0.1587 - sd_loss: 2.4311 - dst_loss: 2.3004 - embed_loss: 63.3169 - val_sparse_categorical_accuracy: 0.1000 - val_student_loss: 2.2154\n",
            "Epoch 2/150\n",
            "75/75 [==============================] - 9s 123ms/step - sparse_categorical_accuracy: 0.1092 - sd_loss: 2.4218 - dst_loss: 2.3029 - embed_loss: 17.6479 - val_sparse_categorical_accuracy: 0.1000 - val_student_loss: 2.0285\n",
            "Epoch 3/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.0954 - sd_loss: 2.4090 - dst_loss: 2.3029 - embed_loss: 13.2329 - val_sparse_categorical_accuracy: 0.1000 - val_student_loss: 1.9367\n",
            "Epoch 4/150\n",
            "75/75 [==============================] - 10s 133ms/step - sparse_categorical_accuracy: 0.1054 - sd_loss: 2.4109 - dst_loss: 2.3029 - embed_loss: 12.1089 - val_sparse_categorical_accuracy: 0.1367 - val_student_loss: 1.8594\n",
            "Epoch 5/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.1096 - sd_loss: 2.4012 - dst_loss: 2.3028 - embed_loss: 11.5367 - val_sparse_categorical_accuracy: 0.1000 - val_student_loss: 1.7829\n",
            "Epoch 6/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.1117 - sd_loss: 2.3997 - dst_loss: 2.3026 - embed_loss: 11.1842 - val_sparse_categorical_accuracy: 0.1367 - val_student_loss: 1.8180\n",
            "Epoch 7/150\n",
            "75/75 [==============================] - 10s 135ms/step - sparse_categorical_accuracy: 0.1142 - sd_loss: 2.3790 - dst_loss: 2.3026 - embed_loss: 10.9994 - val_sparse_categorical_accuracy: 0.1267 - val_student_loss: 1.8140\n",
            "Epoch 8/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.1283 - sd_loss: 2.3641 - dst_loss: 2.3023 - embed_loss: 10.7138 - val_sparse_categorical_accuracy: 0.1300 - val_student_loss: 1.7909\n",
            "Epoch 9/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.1292 - sd_loss: 2.3655 - dst_loss: 2.3022 - embed_loss: 10.4812 - val_sparse_categorical_accuracy: 0.1200 - val_student_loss: 1.7259\n",
            "Epoch 10/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.1325 - sd_loss: 2.3460 - dst_loss: 2.3021 - embed_loss: 10.4031 - val_sparse_categorical_accuracy: 0.1267 - val_student_loss: 1.7297\n",
            "Epoch 11/150\n",
            "75/75 [==============================] - 10s 133ms/step - sparse_categorical_accuracy: 0.1354 - sd_loss: 2.3445 - dst_loss: 2.3019 - embed_loss: 10.1772 - val_sparse_categorical_accuracy: 0.1300 - val_student_loss: 1.7797\n",
            "Epoch 12/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.1454 - sd_loss: 2.3256 - dst_loss: 2.3017 - embed_loss: 9.9642 - val_sparse_categorical_accuracy: 0.1333 - val_student_loss: 1.7836\n",
            "Epoch 13/150\n",
            "75/75 [==============================] - 10s 133ms/step - sparse_categorical_accuracy: 0.1421 - sd_loss: 2.3228 - dst_loss: 2.3015 - embed_loss: 9.8606 - val_sparse_categorical_accuracy: 0.1333 - val_student_loss: 1.7507\n",
            "Epoch 14/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.1479 - sd_loss: 2.3077 - dst_loss: 2.3013 - embed_loss: 9.6713 - val_sparse_categorical_accuracy: 0.1367 - val_student_loss: 1.8164\n",
            "Epoch 15/150\n",
            "75/75 [==============================] - 9s 123ms/step - sparse_categorical_accuracy: 0.1612 - sd_loss: 2.2916 - dst_loss: 2.3012 - embed_loss: 9.6818 - val_sparse_categorical_accuracy: 0.1467 - val_student_loss: 1.9647\n",
            "Epoch 16/150\n",
            "75/75 [==============================] - 10s 132ms/step - sparse_categorical_accuracy: 0.1683 - sd_loss: 2.2681 - dst_loss: 2.3009 - embed_loss: 9.6173 - val_sparse_categorical_accuracy: 0.1500 - val_student_loss: 1.7962\n",
            "Epoch 17/150\n",
            "75/75 [==============================] - 10s 136ms/step - sparse_categorical_accuracy: 0.1637 - sd_loss: 2.2594 - dst_loss: 2.3007 - embed_loss: 9.4610 - val_sparse_categorical_accuracy: 0.1600 - val_student_loss: 1.7293\n",
            "Epoch 18/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.1746 - sd_loss: 2.2524 - dst_loss: 2.3006 - embed_loss: 9.4451 - val_sparse_categorical_accuracy: 0.1567 - val_student_loss: 1.7019\n",
            "Epoch 19/150\n",
            "75/75 [==============================] - 9s 121ms/step - sparse_categorical_accuracy: 0.1921 - sd_loss: 2.2178 - dst_loss: 2.3003 - embed_loss: 9.3375 - val_sparse_categorical_accuracy: 0.1933 - val_student_loss: 1.6818\n",
            "Epoch 20/150\n",
            "75/75 [==============================] - 10s 132ms/step - sparse_categorical_accuracy: 0.1988 - sd_loss: 2.2095 - dst_loss: 2.3000 - embed_loss: 9.1867 - val_sparse_categorical_accuracy: 0.1567 - val_student_loss: 1.7377\n",
            "Epoch 21/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.2054 - sd_loss: 2.1906 - dst_loss: 2.2997 - embed_loss: 9.1609 - val_sparse_categorical_accuracy: 0.1667 - val_student_loss: 1.6784\n",
            "Epoch 22/150\n",
            "75/75 [==============================] - 9s 121ms/step - sparse_categorical_accuracy: 0.2146 - sd_loss: 2.1758 - dst_loss: 2.2995 - embed_loss: 9.1098 - val_sparse_categorical_accuracy: 0.1600 - val_student_loss: 1.6907\n",
            "Epoch 23/150\n",
            "75/75 [==============================] - 9s 123ms/step - sparse_categorical_accuracy: 0.2150 - sd_loss: 2.1553 - dst_loss: 2.2992 - embed_loss: 9.0332 - val_sparse_categorical_accuracy: 0.1800 - val_student_loss: 1.7088\n",
            "Epoch 24/150\n",
            "75/75 [==============================] - 9s 121ms/step - sparse_categorical_accuracy: 0.2133 - sd_loss: 2.1283 - dst_loss: 2.2989 - embed_loss: 8.9384 - val_sparse_categorical_accuracy: 0.1900 - val_student_loss: 1.7245\n",
            "Epoch 25/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.2329 - sd_loss: 2.1207 - dst_loss: 2.2988 - embed_loss: 8.9635 - val_sparse_categorical_accuracy: 0.1900 - val_student_loss: 1.7558\n",
            "Epoch 26/150\n",
            "75/75 [==============================] - 10s 132ms/step - sparse_categorical_accuracy: 0.2417 - sd_loss: 2.0940 - dst_loss: 2.2986 - embed_loss: 8.9711 - val_sparse_categorical_accuracy: 0.2133 - val_student_loss: 1.7296\n",
            "Epoch 27/150\n",
            "75/75 [==============================] - 9s 121ms/step - sparse_categorical_accuracy: 0.2404 - sd_loss: 2.0575 - dst_loss: 2.2980 - embed_loss: 8.8197 - val_sparse_categorical_accuracy: 0.2200 - val_student_loss: 1.6985\n",
            "Epoch 28/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.2567 - sd_loss: 2.0339 - dst_loss: 2.2977 - embed_loss: 8.7884 - val_sparse_categorical_accuracy: 0.2167 - val_student_loss: 1.6003\n",
            "Epoch 29/150\n",
            "75/75 [==============================] - 10s 131ms/step - sparse_categorical_accuracy: 0.2617 - sd_loss: 1.9934 - dst_loss: 2.2971 - embed_loss: 8.7631 - val_sparse_categorical_accuracy: 0.2267 - val_student_loss: 1.5270\n",
            "Epoch 30/150\n",
            "75/75 [==============================] - 10s 132ms/step - sparse_categorical_accuracy: 0.2933 - sd_loss: 1.9797 - dst_loss: 2.2968 - embed_loss: 8.7449 - val_sparse_categorical_accuracy: 0.2400 - val_student_loss: 1.4874\n",
            "Epoch 31/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.3067 - sd_loss: 1.9597 - dst_loss: 2.2965 - embed_loss: 8.6452 - val_sparse_categorical_accuracy: 0.2300 - val_student_loss: 1.5546\n",
            "Epoch 32/150\n",
            "75/75 [==============================] - 9s 122ms/step - sparse_categorical_accuracy: 0.3096 - sd_loss: 1.9172 - dst_loss: 2.2960 - embed_loss: 8.6261 - val_sparse_categorical_accuracy: 0.2533 - val_student_loss: 1.6863\n",
            "Epoch 33/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.3096 - sd_loss: 1.8906 - dst_loss: 2.2955 - embed_loss: 8.5991 - val_sparse_categorical_accuracy: 0.2267 - val_student_loss: 1.6810\n",
            "Epoch 34/150\n",
            "75/75 [==============================] - 9s 123ms/step - sparse_categorical_accuracy: 0.3450 - sd_loss: 1.8575 - dst_loss: 2.2949 - embed_loss: 8.5125 - val_sparse_categorical_accuracy: 0.2867 - val_student_loss: 1.4735\n",
            "Epoch 35/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.3450 - sd_loss: 1.8338 - dst_loss: 2.2946 - embed_loss: 8.4793 - val_sparse_categorical_accuracy: 0.2833 - val_student_loss: 1.6266\n",
            "Epoch 36/150\n",
            "75/75 [==============================] - 9s 128ms/step - sparse_categorical_accuracy: 0.3667 - sd_loss: 1.7850 - dst_loss: 2.2938 - embed_loss: 8.4844 - val_sparse_categorical_accuracy: 0.3067 - val_student_loss: 1.6937\n",
            "Epoch 37/150\n",
            "75/75 [==============================] - 10s 132ms/step - sparse_categorical_accuracy: 0.3837 - sd_loss: 1.7567 - dst_loss: 2.2933 - embed_loss: 8.4093 - val_sparse_categorical_accuracy: 0.3400 - val_student_loss: 1.6344\n",
            "Epoch 38/150\n",
            "75/75 [==============================] - 9s 123ms/step - sparse_categorical_accuracy: 0.3921 - sd_loss: 1.7210 - dst_loss: 2.2928 - embed_loss: 8.4154 - val_sparse_categorical_accuracy: 0.3133 - val_student_loss: 1.6779\n",
            "Epoch 39/150\n",
            "75/75 [==============================] - 10s 133ms/step - sparse_categorical_accuracy: 0.3829 - sd_loss: 1.6999 - dst_loss: 2.2922 - embed_loss: 8.3935 - val_sparse_categorical_accuracy: 0.3133 - val_student_loss: 1.6536\n",
            "Epoch 40/150\n",
            "75/75 [==============================] - 10s 135ms/step - sparse_categorical_accuracy: 0.4075 - sd_loss: 1.6747 - dst_loss: 2.2920 - embed_loss: 8.3631 - val_sparse_categorical_accuracy: 0.4067 - val_student_loss: 1.5063\n",
            "Epoch 41/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.4238 - sd_loss: 1.6356 - dst_loss: 2.2911 - embed_loss: 8.2906 - val_sparse_categorical_accuracy: 0.4200 - val_student_loss: 1.5038\n",
            "Epoch 42/150\n",
            "75/75 [==============================] - 10s 131ms/step - sparse_categorical_accuracy: 0.4462 - sd_loss: 1.6051 - dst_loss: 2.2907 - embed_loss: 8.2756 - val_sparse_categorical_accuracy: 0.5000 - val_student_loss: 1.4633\n",
            "Epoch 43/150\n",
            "75/75 [==============================] - 10s 133ms/step - sparse_categorical_accuracy: 0.4808 - sd_loss: 1.5543 - dst_loss: 2.2897 - embed_loss: 8.2302 - val_sparse_categorical_accuracy: 0.4567 - val_student_loss: 1.4959\n",
            "Epoch 44/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.4762 - sd_loss: 1.5461 - dst_loss: 2.2895 - embed_loss: 8.1946 - val_sparse_categorical_accuracy: 0.4833 - val_student_loss: 1.4364\n",
            "Epoch 45/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.4946 - sd_loss: 1.4889 - dst_loss: 2.2887 - embed_loss: 8.1442 - val_sparse_categorical_accuracy: 0.4800 - val_student_loss: 1.4413\n",
            "Epoch 46/150\n",
            "75/75 [==============================] - 10s 131ms/step - sparse_categorical_accuracy: 0.5067 - sd_loss: 1.4759 - dst_loss: 2.2883 - embed_loss: 8.0977 - val_sparse_categorical_accuracy: 0.4867 - val_student_loss: 1.3536\n",
            "Epoch 47/150\n",
            "75/75 [==============================] - 9s 125ms/step - sparse_categorical_accuracy: 0.4938 - sd_loss: 1.4648 - dst_loss: 2.2876 - embed_loss: 8.1137 - val_sparse_categorical_accuracy: 0.5067 - val_student_loss: 1.4755\n",
            "Epoch 48/150\n",
            "75/75 [==============================] - 10s 133ms/step - sparse_categorical_accuracy: 0.5038 - sd_loss: 1.4274 - dst_loss: 2.2872 - embed_loss: 8.1323 - val_sparse_categorical_accuracy: 0.5100 - val_student_loss: 1.1514\n",
            "Epoch 49/150\n",
            "75/75 [==============================] - 10s 133ms/step - sparse_categorical_accuracy: 0.5204 - sd_loss: 1.4221 - dst_loss: 2.2868 - embed_loss: 8.0906 - val_sparse_categorical_accuracy: 0.5167 - val_student_loss: 1.6402\n",
            "Epoch 50/150\n",
            "75/75 [==============================] - 10s 136ms/step - sparse_categorical_accuracy: 0.5242 - sd_loss: 1.3990 - dst_loss: 2.2863 - embed_loss: 8.1241 - val_sparse_categorical_accuracy: 0.5100 - val_student_loss: 1.3162\n",
            "Epoch 51/150\n",
            "75/75 [==============================] - 10s 130ms/step - sparse_categorical_accuracy: 0.5596 - sd_loss: 1.3508 - dst_loss: 2.2851 - embed_loss: 7.9918 - val_sparse_categorical_accuracy: 0.4867 - val_student_loss: 1.1542\n",
            "Epoch 52/150\n",
            "75/75 [==============================] - 10s 131ms/step - sparse_categorical_accuracy: 0.5546 - sd_loss: 1.3328 - dst_loss: 2.2844 - embed_loss: 7.9353 - val_sparse_categorical_accuracy: 0.5500 - val_student_loss: 1.1987\n",
            "Epoch 53/150\n",
            "75/75 [==============================] - 9s 128ms/step - sparse_categorical_accuracy: 0.5708 - sd_loss: 1.2920 - dst_loss: 2.2838 - embed_loss: 7.9541 - val_sparse_categorical_accuracy: 0.5667 - val_student_loss: 1.1391\n",
            "Epoch 54/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.5746 - sd_loss: 1.2783 - dst_loss: 2.2830 - embed_loss: 7.8988 - val_sparse_categorical_accuracy: 0.5133 - val_student_loss: 1.2339\n",
            "Epoch 55/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.5896 - sd_loss: 1.2636 - dst_loss: 2.2826 - embed_loss: 7.9076 - val_sparse_categorical_accuracy: 0.5800 - val_student_loss: 1.3753\n",
            "Epoch 56/150\n",
            "75/75 [==============================] - 10s 133ms/step - sparse_categorical_accuracy: 0.5850 - sd_loss: 1.2310 - dst_loss: 2.2818 - embed_loss: 7.9220 - val_sparse_categorical_accuracy: 0.5100 - val_student_loss: 1.3055\n",
            "Epoch 57/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.5833 - sd_loss: 1.2489 - dst_loss: 2.2818 - embed_loss: 7.9021 - val_sparse_categorical_accuracy: 0.5800 - val_student_loss: 1.1304\n",
            "Epoch 58/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.6067 - sd_loss: 1.1899 - dst_loss: 2.2808 - embed_loss: 7.8246 - val_sparse_categorical_accuracy: 0.5867 - val_student_loss: 0.9763\n",
            "Epoch 59/150\n",
            "75/75 [==============================] - 9s 122ms/step - sparse_categorical_accuracy: 0.6346 - sd_loss: 1.1494 - dst_loss: 2.2800 - embed_loss: 7.7904 - val_sparse_categorical_accuracy: 0.5500 - val_student_loss: 1.1581\n",
            "Epoch 60/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.6288 - sd_loss: 1.1549 - dst_loss: 2.2796 - embed_loss: 7.8251 - val_sparse_categorical_accuracy: 0.5700 - val_student_loss: 1.1682\n",
            "Epoch 61/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.6221 - sd_loss: 1.1493 - dst_loss: 2.2791 - embed_loss: 7.7960 - val_sparse_categorical_accuracy: 0.6233 - val_student_loss: 1.0294\n",
            "Epoch 62/150\n",
            "75/75 [==============================] - 10s 136ms/step - sparse_categorical_accuracy: 0.6467 - sd_loss: 1.0992 - dst_loss: 2.2782 - embed_loss: 7.7633 - val_sparse_categorical_accuracy: 0.6533 - val_student_loss: 1.0663\n",
            "Epoch 63/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.6479 - sd_loss: 1.0875 - dst_loss: 2.2777 - embed_loss: 7.7657 - val_sparse_categorical_accuracy: 0.6367 - val_student_loss: 1.3881\n",
            "Epoch 64/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.6604 - sd_loss: 1.0696 - dst_loss: 2.2769 - embed_loss: 7.7015 - val_sparse_categorical_accuracy: 0.6233 - val_student_loss: 1.0695\n",
            "Epoch 65/150\n",
            "75/75 [==============================] - 10s 132ms/step - sparse_categorical_accuracy: 0.6567 - sd_loss: 1.0686 - dst_loss: 2.2769 - embed_loss: 7.7145 - val_sparse_categorical_accuracy: 0.6233 - val_student_loss: 0.8741\n",
            "Epoch 66/150\n",
            "75/75 [==============================] - 10s 136ms/step - sparse_categorical_accuracy: 0.6650 - sd_loss: 1.0613 - dst_loss: 2.2762 - embed_loss: 7.7034 - val_sparse_categorical_accuracy: 0.6400 - val_student_loss: 0.9178\n",
            "Epoch 67/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.6792 - sd_loss: 1.0145 - dst_loss: 2.2754 - embed_loss: 7.6826 - val_sparse_categorical_accuracy: 0.6500 - val_student_loss: 0.9629\n",
            "Epoch 68/150\n",
            "75/75 [==============================] - 10s 132ms/step - sparse_categorical_accuracy: 0.6804 - sd_loss: 1.0044 - dst_loss: 2.2743 - embed_loss: 7.6801 - val_sparse_categorical_accuracy: 0.6767 - val_student_loss: 0.9575\n",
            "Epoch 69/150\n",
            "75/75 [==============================] - 9s 125ms/step - sparse_categorical_accuracy: 0.6829 - sd_loss: 0.9768 - dst_loss: 2.2737 - embed_loss: 7.6591 - val_sparse_categorical_accuracy: 0.6800 - val_student_loss: 1.0491\n",
            "Epoch 70/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.7146 - sd_loss: 0.9464 - dst_loss: 2.2729 - embed_loss: 7.5823 - val_sparse_categorical_accuracy: 0.6300 - val_student_loss: 1.1543\n",
            "Epoch 71/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.6971 - sd_loss: 0.9507 - dst_loss: 2.2726 - embed_loss: 7.5658 - val_sparse_categorical_accuracy: 0.6667 - val_student_loss: 0.9020\n",
            "Epoch 72/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.6938 - sd_loss: 0.9506 - dst_loss: 2.2721 - embed_loss: 7.6045 - val_sparse_categorical_accuracy: 0.6800 - val_student_loss: 0.9292\n",
            "Epoch 73/150\n",
            "75/75 [==============================] - 10s 133ms/step - sparse_categorical_accuracy: 0.7125 - sd_loss: 0.9103 - dst_loss: 2.2713 - embed_loss: 7.5602 - val_sparse_categorical_accuracy: 0.6933 - val_student_loss: 0.8691\n",
            "Epoch 74/150\n",
            "75/75 [==============================] - 10s 135ms/step - sparse_categorical_accuracy: 0.7063 - sd_loss: 0.9057 - dst_loss: 2.2707 - embed_loss: 7.5638 - val_sparse_categorical_accuracy: 0.7133 - val_student_loss: 0.8796\n",
            "Epoch 75/150\n",
            "75/75 [==============================] - 9s 125ms/step - sparse_categorical_accuracy: 0.7271 - sd_loss: 0.8823 - dst_loss: 2.2699 - embed_loss: 7.5551 - val_sparse_categorical_accuracy: 0.6733 - val_student_loss: 0.8143\n",
            "Epoch 76/150\n",
            "75/75 [==============================] - 9s 122ms/step - sparse_categorical_accuracy: 0.7267 - sd_loss: 0.8739 - dst_loss: 2.2696 - embed_loss: 7.5423 - val_sparse_categorical_accuracy: 0.7033 - val_student_loss: 0.8739\n",
            "Epoch 77/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.7292 - sd_loss: 0.8627 - dst_loss: 2.2690 - embed_loss: 7.4972 - val_sparse_categorical_accuracy: 0.7000 - val_student_loss: 0.6936\n",
            "Epoch 78/150\n",
            "75/75 [==============================] - 10s 135ms/step - sparse_categorical_accuracy: 0.7300 - sd_loss: 0.8453 - dst_loss: 2.2683 - embed_loss: 7.5051 - val_sparse_categorical_accuracy: 0.7200 - val_student_loss: 0.6695\n",
            "Epoch 79/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.7400 - sd_loss: 0.8299 - dst_loss: 2.2675 - embed_loss: 7.4771 - val_sparse_categorical_accuracy: 0.7100 - val_student_loss: 0.6070\n",
            "Epoch 80/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.7421 - sd_loss: 0.8310 - dst_loss: 2.2675 - embed_loss: 7.4932 - val_sparse_categorical_accuracy: 0.7367 - val_student_loss: 0.6618\n",
            "Epoch 81/150\n",
            "75/75 [==============================] - 10s 129ms/step - sparse_categorical_accuracy: 0.7500 - sd_loss: 0.8014 - dst_loss: 2.2664 - embed_loss: 7.4705 - val_sparse_categorical_accuracy: 0.7467 - val_student_loss: 0.7465\n",
            "Epoch 82/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.7629 - sd_loss: 0.7778 - dst_loss: 2.2652 - embed_loss: 7.4258 - val_sparse_categorical_accuracy: 0.7400 - val_student_loss: 0.7826\n",
            "Epoch 83/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.7708 - sd_loss: 0.7662 - dst_loss: 2.2646 - embed_loss: 7.4207 - val_sparse_categorical_accuracy: 0.7233 - val_student_loss: 0.8441\n",
            "Epoch 84/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.7592 - sd_loss: 0.7743 - dst_loss: 2.2645 - embed_loss: 7.3932 - val_sparse_categorical_accuracy: 0.7567 - val_student_loss: 0.7400\n",
            "Epoch 85/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.7733 - sd_loss: 0.7490 - dst_loss: 2.2646 - embed_loss: 7.3897 - val_sparse_categorical_accuracy: 0.7567 - val_student_loss: 0.7664\n",
            "Epoch 86/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.7708 - sd_loss: 0.7276 - dst_loss: 2.2632 - embed_loss: 7.4248 - val_sparse_categorical_accuracy: 0.7433 - val_student_loss: 0.7835\n",
            "Epoch 87/150\n",
            "75/75 [==============================] - 9s 128ms/step - sparse_categorical_accuracy: 0.7825 - sd_loss: 0.7297 - dst_loss: 2.2628 - embed_loss: 7.3216 - val_sparse_categorical_accuracy: 0.7567 - val_student_loss: 0.6650\n",
            "Epoch 88/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.7896 - sd_loss: 0.7013 - dst_loss: 2.2607 - embed_loss: 7.3887 - val_sparse_categorical_accuracy: 0.7867 - val_student_loss: 0.6607\n",
            "Epoch 89/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.7875 - sd_loss: 0.6911 - dst_loss: 2.2612 - embed_loss: 7.4323 - val_sparse_categorical_accuracy: 0.7433 - val_student_loss: 0.5363\n",
            "Epoch 90/150\n",
            "75/75 [==============================] - 9s 124ms/step - sparse_categorical_accuracy: 0.7875 - sd_loss: 0.7017 - dst_loss: 2.2609 - embed_loss: 7.3967 - val_sparse_categorical_accuracy: 0.7467 - val_student_loss: 0.6369\n",
            "Epoch 91/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.8042 - sd_loss: 0.6755 - dst_loss: 2.2601 - embed_loss: 7.3657 - val_sparse_categorical_accuracy: 0.7733 - val_student_loss: 0.5644\n",
            "Epoch 92/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.7862 - sd_loss: 0.6909 - dst_loss: 2.2593 - embed_loss: 7.2902 - val_sparse_categorical_accuracy: 0.7500 - val_student_loss: 0.4432\n",
            "Epoch 93/150\n",
            "75/75 [==============================] - 9s 125ms/step - sparse_categorical_accuracy: 0.7983 - sd_loss: 0.6691 - dst_loss: 2.2589 - embed_loss: 7.3616 - val_sparse_categorical_accuracy: 0.7467 - val_student_loss: 0.4301\n",
            "Epoch 94/150\n",
            "75/75 [==============================] - 10s 136ms/step - sparse_categorical_accuracy: 0.8025 - sd_loss: 0.6479 - dst_loss: 2.2581 - embed_loss: 7.2510 - val_sparse_categorical_accuracy: 0.7833 - val_student_loss: 0.5197\n",
            "Epoch 95/150\n",
            "75/75 [==============================] - 10s 129ms/step - sparse_categorical_accuracy: 0.8121 - sd_loss: 0.6195 - dst_loss: 2.2578 - embed_loss: 7.2947 - val_sparse_categorical_accuracy: 0.7800 - val_student_loss: 0.4757\n",
            "Epoch 96/150\n",
            "75/75 [==============================] - 10s 140ms/step - sparse_categorical_accuracy: 0.8112 - sd_loss: 0.6178 - dst_loss: 2.2566 - embed_loss: 7.2327 - val_sparse_categorical_accuracy: 0.7900 - val_student_loss: 0.5604\n",
            "Epoch 97/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8288 - sd_loss: 0.5971 - dst_loss: 2.2562 - embed_loss: 7.3144 - val_sparse_categorical_accuracy: 0.8100 - val_student_loss: 0.4363\n",
            "Epoch 98/150\n",
            "75/75 [==============================] - 10s 130ms/step - sparse_categorical_accuracy: 0.8221 - sd_loss: 0.5966 - dst_loss: 2.2552 - embed_loss: 7.1825 - val_sparse_categorical_accuracy: 0.7833 - val_student_loss: 0.4454\n",
            "Epoch 99/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8108 - sd_loss: 0.6180 - dst_loss: 2.2558 - embed_loss: 7.2647 - val_sparse_categorical_accuracy: 0.7967 - val_student_loss: 0.6481\n",
            "Epoch 100/150\n",
            "75/75 [==============================] - 10s 139ms/step - sparse_categorical_accuracy: 0.8238 - sd_loss: 0.5888 - dst_loss: 2.2542 - embed_loss: 7.2118 - val_sparse_categorical_accuracy: 0.7833 - val_student_loss: 0.5293\n",
            "Epoch 101/150\n",
            "75/75 [==============================] - 10s 128ms/step - sparse_categorical_accuracy: 0.8258 - sd_loss: 0.5947 - dst_loss: 2.2539 - embed_loss: 7.2952 - val_sparse_categorical_accuracy: 0.7900 - val_student_loss: 0.4746\n",
            "Epoch 102/150\n",
            "75/75 [==============================] - 10s 136ms/step - sparse_categorical_accuracy: 0.8271 - sd_loss: 0.5598 - dst_loss: 2.2531 - embed_loss: 7.2498 - val_sparse_categorical_accuracy: 0.8400 - val_student_loss: 0.7368\n",
            "Epoch 103/150\n",
            "75/75 [==============================] - 10s 129ms/step - sparse_categorical_accuracy: 0.8421 - sd_loss: 0.5550 - dst_loss: 2.2518 - embed_loss: 7.1514 - val_sparse_categorical_accuracy: 0.8000 - val_student_loss: 0.4472\n",
            "Epoch 104/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8450 - sd_loss: 0.5526 - dst_loss: 2.2517 - embed_loss: 7.1829 - val_sparse_categorical_accuracy: 0.8133 - val_student_loss: 0.4407\n",
            "Epoch 105/150\n",
            "75/75 [==============================] - 10s 139ms/step - sparse_categorical_accuracy: 0.8533 - sd_loss: 0.5339 - dst_loss: 2.2510 - embed_loss: 7.1660 - val_sparse_categorical_accuracy: 0.8300 - val_student_loss: 0.4857\n",
            "Epoch 106/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.8321 - sd_loss: 0.5524 - dst_loss: 2.2503 - embed_loss: 7.2370 - val_sparse_categorical_accuracy: 0.8133 - val_student_loss: 0.6356\n",
            "Epoch 107/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8512 - sd_loss: 0.5188 - dst_loss: 2.2495 - embed_loss: 7.1496 - val_sparse_categorical_accuracy: 0.8200 - val_student_loss: 0.4619\n",
            "Epoch 108/150\n",
            "75/75 [==============================] - 10s 136ms/step - sparse_categorical_accuracy: 0.8400 - sd_loss: 0.5256 - dst_loss: 2.2492 - embed_loss: 7.2078 - val_sparse_categorical_accuracy: 0.8167 - val_student_loss: 0.5651\n",
            "Epoch 109/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.8550 - sd_loss: 0.4999 - dst_loss: 2.2484 - embed_loss: 7.1319 - val_sparse_categorical_accuracy: 0.8067 - val_student_loss: 0.5377\n",
            "Epoch 110/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.8354 - sd_loss: 0.5150 - dst_loss: 2.2469 - embed_loss: 7.1228 - val_sparse_categorical_accuracy: 0.8367 - val_student_loss: 0.4629\n",
            "Epoch 111/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.8400 - sd_loss: 0.5056 - dst_loss: 2.2474 - embed_loss: 7.1502 - val_sparse_categorical_accuracy: 0.8333 - val_student_loss: 0.6939\n",
            "Epoch 112/150\n",
            "75/75 [==============================] - 10s 128ms/step - sparse_categorical_accuracy: 0.8617 - sd_loss: 0.4621 - dst_loss: 2.2455 - embed_loss: 7.1078 - val_sparse_categorical_accuracy: 0.8467 - val_student_loss: 0.4843\n",
            "Epoch 113/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8621 - sd_loss: 0.4874 - dst_loss: 2.2460 - embed_loss: 7.1764 - val_sparse_categorical_accuracy: 0.8267 - val_student_loss: 0.4090\n",
            "Epoch 114/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8592 - sd_loss: 0.4653 - dst_loss: 2.2450 - embed_loss: 7.1292 - val_sparse_categorical_accuracy: 0.8233 - val_student_loss: 0.5432\n",
            "Epoch 115/150\n",
            "75/75 [==============================] - 10s 141ms/step - sparse_categorical_accuracy: 0.8667 - sd_loss: 0.4611 - dst_loss: 2.2445 - embed_loss: 7.1281 - val_sparse_categorical_accuracy: 0.8133 - val_student_loss: 0.3595\n",
            "Epoch 116/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8725 - sd_loss: 0.4502 - dst_loss: 2.2441 - embed_loss: 7.1196 - val_sparse_categorical_accuracy: 0.8133 - val_student_loss: 0.2655\n",
            "Epoch 117/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.8704 - sd_loss: 0.4554 - dst_loss: 2.2436 - embed_loss: 7.1677 - val_sparse_categorical_accuracy: 0.8367 - val_student_loss: 0.3719\n",
            "Epoch 118/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.8692 - sd_loss: 0.4469 - dst_loss: 2.2440 - embed_loss: 7.1356 - val_sparse_categorical_accuracy: 0.8333 - val_student_loss: 0.3816\n",
            "Epoch 119/150\n",
            "75/75 [==============================] - 9s 125ms/step - sparse_categorical_accuracy: 0.8712 - sd_loss: 0.4398 - dst_loss: 2.2422 - embed_loss: 7.0619 - val_sparse_categorical_accuracy: 0.8533 - val_student_loss: 0.5176\n",
            "Epoch 120/150\n",
            "75/75 [==============================] - 10s 135ms/step - sparse_categorical_accuracy: 0.8817 - sd_loss: 0.4187 - dst_loss: 2.2406 - embed_loss: 7.0824 - val_sparse_categorical_accuracy: 0.8367 - val_student_loss: 0.3418\n",
            "Epoch 121/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.8879 - sd_loss: 0.4177 - dst_loss: 2.2405 - embed_loss: 7.0939 - val_sparse_categorical_accuracy: 0.8333 - val_student_loss: 0.4164\n",
            "Epoch 122/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8725 - sd_loss: 0.4322 - dst_loss: 2.2399 - embed_loss: 7.0604 - val_sparse_categorical_accuracy: 0.8233 - val_student_loss: 0.3187\n",
            "Epoch 123/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8838 - sd_loss: 0.4040 - dst_loss: 2.2386 - embed_loss: 7.0458 - val_sparse_categorical_accuracy: 0.8433 - val_student_loss: 0.3136\n",
            "Epoch 124/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.8921 - sd_loss: 0.3894 - dst_loss: 2.2379 - embed_loss: 7.0504 - val_sparse_categorical_accuracy: 0.8533 - val_student_loss: 0.3595\n",
            "Epoch 125/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.8888 - sd_loss: 0.3816 - dst_loss: 2.2368 - embed_loss: 7.0592 - val_sparse_categorical_accuracy: 0.8633 - val_student_loss: 0.4258\n",
            "Epoch 126/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.8904 - sd_loss: 0.3914 - dst_loss: 2.2376 - embed_loss: 7.1034 - val_sparse_categorical_accuracy: 0.8500 - val_student_loss: 0.4078\n",
            "Epoch 127/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8917 - sd_loss: 0.3815 - dst_loss: 2.2364 - embed_loss: 6.9929 - val_sparse_categorical_accuracy: 0.8467 - val_student_loss: 0.3265\n",
            "Epoch 128/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.8808 - sd_loss: 0.3861 - dst_loss: 2.2360 - embed_loss: 6.9986 - val_sparse_categorical_accuracy: 0.8867 - val_student_loss: 0.4264\n",
            "Epoch 129/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.8892 - sd_loss: 0.3708 - dst_loss: 2.2358 - embed_loss: 7.0311 - val_sparse_categorical_accuracy: 0.8433 - val_student_loss: 0.4462\n",
            "Epoch 130/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.8879 - sd_loss: 0.3675 - dst_loss: 2.2334 - embed_loss: 7.1153 - val_sparse_categorical_accuracy: 0.8300 - val_student_loss: 0.4611\n",
            "Epoch 131/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.8933 - sd_loss: 0.3653 - dst_loss: 2.2339 - embed_loss: 7.0051 - val_sparse_categorical_accuracy: 0.8600 - val_student_loss: 0.2608\n",
            "Epoch 132/150\n",
            "75/75 [==============================] - 10s 139ms/step - sparse_categorical_accuracy: 0.9000 - sd_loss: 0.3566 - dst_loss: 2.2329 - embed_loss: 6.9700 - val_sparse_categorical_accuracy: 0.8633 - val_student_loss: 0.2871\n",
            "Epoch 133/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.9042 - sd_loss: 0.3489 - dst_loss: 2.2314 - embed_loss: 6.9827 - val_sparse_categorical_accuracy: 0.8833 - val_student_loss: 0.4312\n",
            "Epoch 134/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.9025 - sd_loss: 0.3487 - dst_loss: 2.2321 - embed_loss: 7.0276 - val_sparse_categorical_accuracy: 0.8433 - val_student_loss: 0.5138\n",
            "Epoch 135/150\n",
            "75/75 [==============================] - 10s 141ms/step - sparse_categorical_accuracy: 0.8946 - sd_loss: 0.3418 - dst_loss: 2.2297 - embed_loss: 7.0088 - val_sparse_categorical_accuracy: 0.8833 - val_student_loss: 0.3215\n",
            "Epoch 136/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.9029 - sd_loss: 0.3325 - dst_loss: 2.2311 - embed_loss: 7.0061 - val_sparse_categorical_accuracy: 0.8667 - val_student_loss: 0.2967\n",
            "Epoch 137/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.9025 - sd_loss: 0.3386 - dst_loss: 2.2286 - embed_loss: 6.9695 - val_sparse_categorical_accuracy: 0.8900 - val_student_loss: 0.4087\n",
            "Epoch 138/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.9025 - sd_loss: 0.3208 - dst_loss: 2.2277 - embed_loss: 6.9843 - val_sparse_categorical_accuracy: 0.8867 - val_student_loss: 0.2376\n",
            "Epoch 139/150\n",
            "75/75 [==============================] - 9s 126ms/step - sparse_categorical_accuracy: 0.8979 - sd_loss: 0.3393 - dst_loss: 2.2275 - embed_loss: 6.9384 - val_sparse_categorical_accuracy: 0.8633 - val_student_loss: 0.1756\n",
            "Epoch 140/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.9062 - sd_loss: 0.3373 - dst_loss: 2.2280 - embed_loss: 6.9800 - val_sparse_categorical_accuracy: 0.8833 - val_student_loss: 0.1457\n",
            "Epoch 141/150\n",
            "75/75 [==============================] - 10s 138ms/step - sparse_categorical_accuracy: 0.9092 - sd_loss: 0.3124 - dst_loss: 2.2259 - embed_loss: 7.0189 - val_sparse_categorical_accuracy: 0.8967 - val_student_loss: 0.3543\n",
            "Epoch 142/150\n",
            "75/75 [==============================] - 10s 139ms/step - sparse_categorical_accuracy: 0.9050 - sd_loss: 0.3305 - dst_loss: 2.2266 - embed_loss: 6.9734 - val_sparse_categorical_accuracy: 0.8667 - val_student_loss: 0.3113\n",
            "Epoch 143/150\n",
            "75/75 [==============================] - 10s 134ms/step - sparse_categorical_accuracy: 0.9133 - sd_loss: 0.2901 - dst_loss: 2.2245 - embed_loss: 6.9817 - val_sparse_categorical_accuracy: 0.8733 - val_student_loss: 0.3944\n",
            "Epoch 144/150\n",
            "75/75 [==============================] - 9s 127ms/step - sparse_categorical_accuracy: 0.9062 - sd_loss: 0.3110 - dst_loss: 2.2246 - embed_loss: 6.9022 - val_sparse_categorical_accuracy: 0.8733 - val_student_loss: 0.2082\n",
            "Epoch 145/150\n",
            "75/75 [==============================] - 10s 135ms/step - sparse_categorical_accuracy: 0.9062 - sd_loss: 0.3200 - dst_loss: 2.2250 - embed_loss: 6.9666 - val_sparse_categorical_accuracy: 0.8767 - val_student_loss: 0.3990\n",
            "Epoch 146/150\n",
            "75/75 [==============================] - 10s 131ms/step - sparse_categorical_accuracy: 0.9233 - sd_loss: 0.2726 - dst_loss: 2.2223 - embed_loss: 6.9315 - val_sparse_categorical_accuracy: 0.8800 - val_student_loss: 0.4402\n",
            "Epoch 147/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.9100 - sd_loss: 0.3005 - dst_loss: 2.2227 - embed_loss: 6.9653 - val_sparse_categorical_accuracy: 0.8600 - val_student_loss: 0.2678\n",
            "Epoch 148/150\n",
            "75/75 [==============================] - 10s 137ms/step - sparse_categorical_accuracy: 0.9117 - sd_loss: 0.2871 - dst_loss: 2.2222 - embed_loss: 6.9965 - val_sparse_categorical_accuracy: 0.8700 - val_student_loss: 0.2320\n",
            "Epoch 149/150\n",
            "75/75 [==============================] - 11s 142ms/step - sparse_categorical_accuracy: 0.9121 - sd_loss: 0.2832 - dst_loss: 2.2217 - embed_loss: 6.9426 - val_sparse_categorical_accuracy: 0.8733 - val_student_loss: 0.3420\n",
            "Epoch 150/150\n",
            "75/75 [==============================] - 9s 128ms/step - sparse_categorical_accuracy: 0.9075 - sd_loss: 0.3078 - dst_loss: 2.2202 - embed_loss: 6.9349 - val_sparse_categorical_accuracy: 0.8700 - val_student_loss: 0.2959\n",
            "10/10 [==============================] - 8s 10ms/step - sparse_categorical_accuracy: 0.8667 - student_loss: 0.3683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8666666746139526, 0.17251946032047272]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWW8KaaIBfwj",
        "outputId": "0f3c1867-6117-4021-f527-24b65a0a6f4f"
      },
      "source": [
        "distiller.evaluate(sec_test_ds.batch(32))\n",
        "\n",
        "#Test on a new Digit never seen by the model\n",
        "\n",
        "label = 0\n",
        "scores = distiller(testing_wav_data_bis)[0]\n",
        "class_scores = tf.reduce_mean(scores, axis=0)\n",
        "#print(class_scores)\n",
        "top_class = tf.argmax(class_scores)\n",
        "print(f'Je trouve {top_class} reponse etait {label}')\n",
        "\n",
        "# for e in sec_test_ds.batch(1):\n",
        "#   label = e[1].numpy()\n",
        "#   scores = distiller(e[0])[0]\n",
        "#   class_scores = tf.reduce_mean(scores, axis=0)\n",
        "#   #print(class_scores)\n",
        "#   top_class = tf.argmax(class_scores)\n",
        "#   print(f'Je trouve {top_class} reponse etait {label}')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 8s 11ms/step - sparse_categorical_accuracy: 0.8667 - student_loss: 0.3683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.8666666746139526, 0.17251946032047272]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5K9IFAIovvCU"
      },
      "source": [
        "## 2. Quantization\n",
        "\n",
        "We use a similar process as the first time to quantize the new distilled model to reduce even more the size of the final **model.cc** file that will need to be embedded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h77i8HrtMlZL",
        "outputId": "adf5d293-6059-4320-a55e-e544af489982"
      },
      "source": [
        "saved_model_path = './reduced_model'\n",
        "\n",
        "input_segment = tf.keras.layers.Input(shape=(None,96,64), dtype=tf.float32, name='audio')\n",
        "output = distiller(input_segment)\n",
        "serving_outputs = output[0]\n",
        "#serving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\n",
        "serving_model = tf.keras.Model(input_segment, serving_outputs)\n",
        "serving_model.save(saved_model_path, include_optimizer=False)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "WARNING:tensorflow:Model was constructed with shape (None, None, 1, 96, 64) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 1, 96, 64), dtype=tf.float32, name='audio'), name='audio', description=\"created by layer 'audio'\"), but it was called on an input with incompatible shape (None, 1, 96, 64).\n",
            "INFO:tensorflow:Assets written to: ./reduced_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DL7Yn-UsNnfU",
        "outputId": "3272349f-d241-481a-c88b-f3a13a2b8290"
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "KD_model_no_quant_tflite = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(KD_MODEL_NO_QUANT_TFLITE, \"wb\").write(KD_model_no_quant_tflite)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "336136"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1kOntnwSurN",
        "outputId": "bc85b286-6d18-42a7-8bcb-3b1c9a0e5313"
      },
      "source": [
        "\n",
        "# Convert the model to the TensorFlow Lite format with quantization\n",
        "print(sec_ds.batch(32).element_spec)\n",
        "def representative_dataset():\n",
        "  for data in sec_ds.batch(32):\n",
        "    yield [tf.dtypes.cast(data[0], tf.float32)]\n",
        "# Set the optimization flag.\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# Enforce integer only quantization\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "# Provide a representative dataset to ensure we quantize correctly.\n",
        "converter.representative_dataset = representative_dataset#representative_dataset\n",
        "KD_model_tflite = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(KD_MODEL_TFLITE, \"wb\").write(KD_model_tflite)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(TensorSpec(shape=(None, 1, 96, 64), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128968"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDoklMhFS_FQ",
        "outputId": "9c5c31a4-cb48-4fca-89e7-195f08bebc40"
      },
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
        "!xxd -i {KD_MODEL_TFLITE} > {KD_MODEL_TFLITE_MICRO}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = KD_MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/g_model/g' {KD_MODEL_TFLITE_MICRO}"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.38)] [Co\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rHit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "\r0% [3 InRelease 15.6 kB/88.7 kB 18%] [Connecting to security.ubuntu.com (91.189\r0% [1 InRelease gpgv 15.9 kB] [3 InRelease 15.6 kB/88.7 kB 18%] [Connecting to \r                                                                               \rHit:5 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [3 InRelease 59.1 kB/88.7 kB 67%] [Connecting to \r                                                                               \rHit:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [3 InRelease 80.8 kB/88.7 kB 91%] [Connecting to \r0% [1 InRelease gpgv 15.9 kB] [Waiting for headers] [Connecting to security.ubu\r                                                                               \rGet:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [7 InRelease 9,844 B/74.6 kB 13%] [Connecting to \r                                                                               \rHit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 15.9 kB] [7 InRelease 60.5 kB/74.6 kB 81%] [Connecting to \r0% [1 InRelease gpgv 15.9 kB] [Connecting to security.ubuntu.com (91.189.91.38)\r                                                                               \rGet:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Fetched 252 kB in 2s (126 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_vE-ZDkHVxe"
      },
      "source": [
        "# V/ Compare Model Performance\n",
        "\n",
        "To prove these models are accurate even after conversion and quantization, we'll compare their predictions and loss on our test dataset.\n",
        "\n",
        "**Helper functions**\n",
        "\n",
        "We define the `predict` (for predictions) and `evaluate` (for loss) functions for TFLite models. *Note: These are already included in a TF model, but not in  a TFLite model.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKtmxEhko1S1",
        "cellView": "code"
      },
      "source": [
        "def predict_tflite(tflite_model, x_test_in):\n",
        "  # Prepare the test data\n",
        "  x_test_ = x_test_in\n",
        "  x_test_ = x_test_.reshape((x_test.size, 1))\n",
        "  x_test_ = x_test_.astype(np.float32)\n",
        "\n",
        "  # Initialize the TFLite interpreter\n",
        "  interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details = interpreter.get_output_details()[0]\n",
        "\n",
        "  # If required, quantize the input layer (from float to integer)\n",
        "  input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "  if (input_scale, input_zero_point) != (0.0, 0):\n",
        "    x_test_ = x_test_ / input_scale + input_zero_point\n",
        "    x_test_ = x_test_.astype(input_details[\"dtype\"])\n",
        "  \n",
        "  # Invoke the interpreter\n",
        "  y_pred = np.empty(x_test_.size, dtype=output_details[\"dtype\"])\n",
        "  for i in range(len(x_test_)):\n",
        "    interpreter.set_tensor(input_details[\"index\"], [x_test_[i]])\n",
        "    interpreter.invoke()\n",
        "    y_pred[i] = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "  \n",
        "  # If required, dequantized the output layer (from integer to float)\n",
        "  output_scale, output_zero_point = output_details[\"quantization\"]\n",
        "  if (output_scale, output_zero_point) != (0.0, 0):\n",
        "    y_pred = y_pred.astype(np.float32)\n",
        "    y_pred = (y_pred - output_zero_point) * output_scale\n",
        "\n",
        "  return y_pred\n",
        "\n",
        "def evaluate_tflite(tflite_model, x_test, y_true):\n",
        "  global model\n",
        "  y_pred = predict_tflite(tflite_model, x_test)\n",
        "  loss_function = tf.keras.losses.get(model.loss)\n",
        "  loss = loss_function(y_true, y_pred).numpy()\n",
        "  return loss"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLZLY0D4gl6U"
      },
      "source": [
        "**1. Predictions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RS3zni1gkrt"
      },
      "source": [
        "# Calculate predictions\n",
        "#"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vlfJqbiZMU"
      },
      "source": [
        "**2. Loss (MSE/Mean Squared Error)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpHifyGZRhw8"
      },
      "source": [
        "# # Calculate loss\n",
        "# loss_tf, _ = model.evaluate(x_test, y_test, verbose=0)\n",
        "# loss_no_quant_tflite = evaluate_tflite(model_no_quant_tflite, x_test, y_test)\n",
        "# loss_tflite = evaluate_tflite(model_tflite, x_test, y_test)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3HLT0UOjTY_"
      },
      "source": [
        "# # Compare loss\n",
        "# df = pd.DataFrame.from_records(\n",
        "#     [[\"TensorFlow\", loss_tf],\n",
        "#      [\"TensorFlow Lite\", loss_no_quant_tflite],\n",
        "#      [\"TensorFlow Lite Quantized\", loss_tflite]],\n",
        "#      columns = [\"Model\", \"Loss/MSE\"], index=\"Model\").round(4)\n",
        "# df"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Vjw7VckLu1"
      },
      "source": [
        "**3. Size**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEXiJ8dFkL2R"
      },
      "source": [
        "# Calculate size original model\n",
        "\n",
        "size_no_quant_tflite = os.path.getsize(MODEL_NO_QUANT_TFLITE)\n",
        "size_tflite = os.path.getsize(MODEL_TFLITE)\n",
        "\n",
        "# Calculate size KD model\n",
        "\n",
        "KD_size_no_quant_tflite = os.path.getsize(KD_MODEL_NO_QUANT_TFLITE)\n",
        "KD_size_tflite = os.path.getsize(KD_MODEL_TFLITE)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DdsCaL7kL4u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "3afea6f9-1fba-4a22-f3b1-aef0e978355b"
      },
      "source": [
        "# Compare size\n",
        "pd.DataFrame.from_records(\n",
        "    [[\"TensorFlow Lite\", f\"{size_no_quant_tflite} bytes \"],\n",
        "     [\"TensorFlow Lite Quantized\", f\"{size_tflite} bytes\", f\"(reduced by {size_no_quant_tflite - size_tflite} bytes)\"],\n",
        "     [\"KD TensorFlow Lite\", f\"{KD_size_no_quant_tflite} bytes \", f\"(reduced by {size_no_quant_tflite - KD_size_no_quant_tflite} bytes)\"],\n",
        "     [\" KD TensorFlow Lite Quantized\", f\"{KD_size_tflite} bytes\", f\"(reduced by {KD_size_no_quant_tflite - KD_size_tflite} bytes)\"]],\n",
        "     columns = [\"Model\", \"Size\", \"\"], index=\"Model\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Size</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Model</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>TensorFlow Lite</th>\n",
              "      <td>13334616 bytes</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TensorFlow Lite Quantized</th>\n",
              "      <td>3649104 bytes</td>\n",
              "      <td>(reduced by 9685512 bytes)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KD TensorFlow Lite</th>\n",
              "      <td>336136 bytes</td>\n",
              "      <td>(reduced by 12998480 bytes)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>KD TensorFlow Lite Quantized</th>\n",
              "      <td>128968 bytes</td>\n",
              "      <td>(reduced by 207168 bytes)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          Size                             \n",
              "Model                                                                      \n",
              "TensorFlow Lite                13334616 bytes                          None\n",
              "TensorFlow Lite Quantized        3649104 bytes   (reduced by 9685512 bytes)\n",
              "KD TensorFlow Lite               336136 bytes   (reduced by 12998480 bytes)\n",
              " KD TensorFlow Lite Quantized     128968 bytes    (reduced by 207168 bytes)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXdmfo7imGMB"
      },
      "source": [
        "**Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvRy0ZyMhQOX"
      },
      "source": [
        "# VI/ Deploy to a Microcontroller\n",
        "\n",
        "Follow the instructions in the [hello_world](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/lite/micro/examples/hello_world) README.md for [TensorFlow Lite for MicroControllers](https://www.tensorflow.org/lite/microcontrollers/overview) to deploy this model on a specific microcontroller.\n",
        "\n",
        "**Reference Model:** If you have not modified this notebook, you can follow the instructions as is, to deploy the model. Refer to the [`hello_world/train/models`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/train/models) directory to access the models generated in this notebook.\n",
        "\n",
        "**New Model:** If you have generated a new model, then update the values assigned to the variables defined in [`hello_world/model.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/model.cc) with values displayed after running the following cell."
      ]
    }
  ]
}